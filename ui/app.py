"""
Gradio Web UI for Semantic-Cluster-WebUI
é›†æˆæ‰€æœ‰æ­¥éª¤çš„Webç•Œé¢

ğŸ“… Last Updated: 2026-01-31
"""

import sys
import os
import subprocess
import platform
import io
import hashlib
import warnings
from pathlib import Path
from typing import Optional
from contextlib import redirect_stdout
from threading import Thread
from queue import Queue, Empty

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# æ¶ˆé™¤ numexpr/bottleneck ç‰ˆæœ¬è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, message=".*numexpr.*")
warnings.filterwarnings("ignore", category=UserWarning, message=".*bottleneck.*")
# æ¶ˆé™¤ VLM åŠ è½½æ—¶çš„æç¤ºï¼šmeta device å‚æ•° offloadã€Qwen2VL å¿«é€Ÿ image processor
warnings.filterwarnings("ignore", category=UserWarning, message=".*meta device.*offloaded.*")
warnings.filterwarnings("ignore", category=UserWarning, message=".*Qwen2VLImageProcessor.*fast processor.*")
# æ¶ˆé™¤ boto3 Python 3.9 å¼ƒç”¨æç¤ºï¼ˆaccelerate ä¾èµ– boto3ï¼Œ2026 å¹´åéœ€ Python 3.10+ï¼‰
try:
    warnings.filterwarnings("ignore", category=DeprecationWarning, module="boto3.compat")
    if hasattr(warnings, "PythonDeprecationWarning"):
        warnings.filterwarnings("ignore", category=warnings.PythonDeprecationWarning, module="boto3.compat")
except Exception:
    pass

import gradio as gr
import json
import numpy as np
import shutil
import yaml

from datetime import datetime
from utils import ConfigLoader


def _embed_cache_key(input_path: Path, index: dict, embedding_provider: str, run_device: str) -> str:
    """è®¡ç®—åµŒå…¥ç¼“å­˜é”®ï¼šè¾“å…¥è·¯å¾„+å›¾åƒè·¯å¾„é›†åˆ+åµŒå…¥é…ç½®ï¼ˆä½¿ç”¨è·¯å¾„è€Œé image_idï¼Œé¿å…é‡å¯åæ‰«æé¡ºåºå˜åŒ–å¯¼è‡´ç´¢å¼•é”™ä½ï¼‰"""
    paths = tuple(sorted(index[k]["path"] for k in index))
    data = f"{input_path}|{embedding_provider}|{run_device}|{paths}"
    return hashlib.sha256(data.encode()).hexdigest()[:16]


def get_latest_organized_dir() -> Optional[Path]:
    """è¿”å›æœ€è¿‘çš„ organized æ–‡ä»¶å¤¹è·¯å¾„ï¼Œä¸å­˜åœ¨åˆ™è¿”å› None"""
    project_root = Path(__file__).parent.parent
    output_base = project_root / "data" / "output"
    if not output_base.exists():
        return None
    candidates = []
    for p in output_base.iterdir():
        if p.is_dir() and not p.name.startswith("."):
            organized = p / "organized"
            if organized.exists() and organized.is_dir():
                candidates.append((organized.stat().st_mtime, organized))
    if not candidates:
        return None
    candidates.sort(key=lambda x: x[0], reverse=True)
    return candidates[0][1]


def open_folder_in_explorer(path: Path) -> bool:
    """åœ¨ç³»ç»Ÿæ–‡ä»¶ç®¡ç†å™¨ä¸­æ‰“å¼€æ–‡ä»¶å¤¹ï¼Œè¿”å›æ˜¯å¦æˆåŠŸ"""
    path = Path(path).resolve()
    if not path.exists() or not path.is_dir():
        return False
    path_str = str(path)
    try:
        if platform.system() == "Windows":
            os.startfile(path_str)
        elif platform.system() == "Darwin":
            subprocess.run(["open", path_str], check=False)
        else:
            subprocess.run(["xdg-open", path_str], check=False)
        return True
    except Exception:
        return False


def open_latest_organized() -> str:
    """æ‰“å¼€æœ€è¿‘çš„ organized æ–‡ä»¶å¤¹ï¼Œè¿”å›çŠ¶æ€æ¶ˆæ¯"""
    latest = get_latest_organized_dir()
    if latest is None:
        return "âŒ æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶å¤¹ (data/output/*/organized/)"
    if open_folder_in_explorer(latest):
        return f"âœ… å·²æ‰“å¼€: {latest}"
    return f"âŒ æ— æ³•æ‰“å¼€: {latest}"


def get_gpu_status() -> str:
    """æ£€æµ‹ PyTorch GPU å¯ç”¨æ€§ï¼Œè¿”å›çŠ¶æ€æ–‡æœ¬"""
    try:
        import torch
        if torch.cuda.is_available():
            name = torch.cuda.get_device_name(0) if torch.cuda.device_count() > 0 else "Unknown"
            count = torch.cuda.device_count()
            cuda_ver = torch.version.cuda or "N/A"
            return f"âœ… GPU å¯ç”¨ | {name} Ã— {count} | CUDA {cuda_ver}"
        return "âš ï¸ GPU ä¸å¯ç”¨ (ä½¿ç”¨ CPU æ¨¡å¼)"
    except Exception as e:
        return f"âŒ æ£€æµ‹å¤±è´¥: {e}"


# é»˜è®¤å‚æ•°ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦å·²ä¿®æ”¹ï¼‰
UI_DEFAULTS = {
    "input_dir": "test_pics",
    "embedding_provider": "dinov2",
    "run_device": "cuda",
    "cluster_backend": "hdbscan",
    "min_samples": 2,
    "epsilon": 1.0,
    "dbscan_metric": "euclidean",
    "dbscan_algorithm": "auto",
    "cluster_selection_method": "leaf",
    "cluster_selection_epsilon": 0.0,
    "cluster_selection_persistence": 0.4,
    "batch_size": 16,
    "caption_mode": "representative",
    "top_k_sampling": 2,
    "vlm_model_scale": "small",
    "caption_batch_size": 4,
    "vlm_quantization": "none",
    "max_image_size": 512,
}
UI_PARAM_NAMES = (
    "input_dir", "embedding_provider", "run_device", "cluster_backend",
    "min_samples", "epsilon", "dbscan_metric", "dbscan_algorithm", "cluster_selection_method",
    "cluster_selection_epsilon", "cluster_selection_persistence",
    "batch_size", "caption_mode", "top_k_sampling", "vlm_model_scale", "caption_batch_size", "vlm_quantization", "max_image_size",
)
UI_PARAM_LABELS = {
    "input_dir": "è¾“å…¥ç›®å½•",
    "embedding_provider": "ç‰¹å¾æ¨¡å‹",
    "run_device": "G10 è¿è¡Œè®¾å¤‡ (åµŒå…¥+VLM)",
    "cluster_backend": "èšç±»ç®—æ³•",
    "min_samples": "Min Samples",
    "epsilon": "Epsilon",
    "dbscan_metric": "DBSCAN åº¦é‡",
    "dbscan_algorithm": "DBSCAN ç®—æ³•",
    "cluster_selection_method": "ç°‡é€‰æ‹©æ–¹æ³•",
    "cluster_selection_epsilon": "Cluster Sel Epsilon",
    "cluster_selection_persistence": "Cluster Sel Persistence",
    "batch_size": "æ‰¹é‡å¤§å°",
    "caption_mode": "æè¿°æ¨¡å¼",
    "top_k_sampling": "D7 Top-K é‡‡æ ·",
    "vlm_model_scale": "D2 æ¨¡å‹è§„æ¨¡",
    "caption_batch_size": "D6 æè¿°æ‰¹é‡",
    "vlm_quantization": "D9 é‡åŒ–",
    "max_image_size": "D10 æœ€å¤§åˆ†è¾¨ç‡",
}


def get_config_choices() -> list:
    """æ‰«æ config/*.yamlï¼Œè¿”å›é…ç½®ååˆ—è¡¨ï¼ˆæ’é™¤ prompts.yamlï¼‰ï¼Œé»˜è®¤å« default_cfg"""
    project_root = Path(__file__).parent.parent
    config_dir = project_root / "config"
    choices = ["default_cfg"]
    if config_dir.exists():
        for f in sorted(config_dir.glob("*.yaml")):
            if f.name != "prompts.yaml" and f.stem not in choices:
                choices.append(f.stem)
    return choices


def reset_to_defaults() -> tuple:
    """è¿”å›é»˜è®¤å‚æ•°å…ƒç»„ï¼Œç”¨äºæ¢å¤é»˜è®¤é…ç½®"""
    return (
        UI_DEFAULTS["input_dir"],
        UI_DEFAULTS["embedding_provider"],
        UI_DEFAULTS["run_device"],
        UI_DEFAULTS["cluster_backend"],
        UI_DEFAULTS["min_samples"],
        UI_DEFAULTS["epsilon"],
        UI_DEFAULTS["dbscan_metric"],
        UI_DEFAULTS["dbscan_algorithm"],
        UI_DEFAULTS["cluster_selection_method"],
        UI_DEFAULTS["cluster_selection_epsilon"],
        UI_DEFAULTS["cluster_selection_persistence"],
        UI_DEFAULTS["batch_size"],
        UI_DEFAULTS["caption_mode"],
        UI_DEFAULTS["top_k_sampling"],
        UI_DEFAULTS["vlm_model_scale"],
        UI_DEFAULTS["caption_batch_size"],
        UI_DEFAULTS["vlm_quantization"],
        UI_DEFAULTS["max_image_size"],
        False,  # force_rerun_step1_2
        1,  # random_seed
        "<span style='font-size:0.9em'><b>å½“å‰å·²ä¿®æ”¹:</b> æ— ï¼ˆå‡ä¸ºé»˜è®¤ï¼‰</span>",
    )


def save_ui_config(
    config_name,
    input_dir, embedding_provider, embedding_device, cluster_backend,
    min_samples, epsilon, dbscan_metric, dbscan_algorithm,
    cluster_selection_method, cluster_selection_epsilon, cluster_selection_persistence,
    batch_size, caption_mode, top_k_sampling, vlm_model_scale, caption_batch_size, vlm_quantization, max_image_size, run_device, force_rerun_step1_2, random_seed,
) -> str:
    """å°†å½“å‰ UI å‚æ•°ä¿å­˜åˆ° config/{config_name}.yaml"""
    try:
        name = (config_name or "default_cfg").strip() or "default_cfg"
        project_root = Path(__file__).parent.parent
        config_dir = project_root / "config"
        target_path = config_dir / f"{name}.yaml"
        base_config = config_dir / "config.yaml"

        # è‹¥ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä» config.yaml åŠ è½½ä½œä¸ºæ¨¡æ¿
        load_path = target_path if target_path.exists() else base_config
        loader = ConfigLoader(config_path=str(load_path))

        input_path = Path((input_dir or "").strip())
        if not input_path.is_absolute():
            input_path = project_root / input_path
        loader.set("data.input_directory", str(input_path))

        loader.set("embedding.provider", embedding_provider)
        loader.set("embedding.device", run_device)
        backbone = "dinov2_vitl14" if embedding_provider == "dinov2" else "clip_vitb16"
        loader.set("embedding.backbone", backbone)
        loader.set("embedding.batch_size", int(batch_size))

        loader.set("clustering.backend", cluster_backend)
        loader.set("clustering.min_samples", int(min_samples))
        loader.set("clustering.epsilon", float(epsilon))
        loader.set("clustering.dbscan_metric", dbscan_metric)
        loader.set("clustering.dbscan_algorithm", dbscan_algorithm)
        loader.set("clustering.cluster_selection_method", cluster_selection_method)
        loader.set("clustering.min_cluster_size", None)
        loader.set("clustering.cluster_selection_epsilon", float(cluster_selection_epsilon))
        loader.set("clustering.cluster_selection_persistence", float(cluster_selection_persistence))
        loader.set("postprocessing.caption_mode", (caption_mode or "representative").strip() or "representative")
        loader.set("postprocessing.top_k_sampling", int(top_k_sampling) if top_k_sampling is not None else 2)
        loader.set("vlm.model_scale", (vlm_model_scale or "small").strip().lower() or "small")
        loader.set("vlm.caption_batch_size", int(caption_batch_size) if caption_batch_size is not None else 4)
        loader.set("vlm.quantization", (vlm_quantization or "none").strip().lower() or "none")
        loader.set("vlm.max_image_size", int(max_image_size) if max_image_size is not None else 512)
        loader.set("vlm.device", (run_device or "cuda").strip() or "cuda")
        loader.set("system.seed", int(random_seed) if random_seed is not None and int(random_seed) >= 0 else -1)

        loader.save_config(output_path=str(target_path))
        return f"âœ… å·²ä¿å­˜åˆ° config/{name}.yaml"
    except Exception as e:
        return f"âŒ ä¿å­˜å¤±è´¥: {str(e)}"


def load_ui_config(
    config_name,
    input_dir, embedding_provider, embedding_device, cluster_backend,
    min_samples, epsilon, dbscan_metric, dbscan_algorithm,
    cluster_selection_method, cluster_selection_epsilon, cluster_selection_persistence,
    batch_size, caption_mode, top_k_sampling, vlm_model_scale, caption_batch_size, vlm_quantization, max_image_size, run_device, force_rerun_step1_2, random_seed,
) -> tuple:
    """ä» config/{config_name}.yaml è¯»å–é…ç½®å¹¶è¿”å› UI å‚æ•°å…ƒç»„ï¼›å¤±è´¥æ—¶è¿”å›å½“å‰å€¼å¹¶å¸¦é”™è¯¯ä¿¡æ¯"""
    fallback = (
        (input_dir or "").strip(), embedding_provider, (run_device or "cuda").strip() or "cuda", cluster_backend,
        int(min_samples), float(epsilon), dbscan_metric, dbscan_algorithm, cluster_selection_method,
        float(cluster_selection_epsilon), float(cluster_selection_persistence),
        int(batch_size), (caption_mode or "representative").strip() or "representative",
        int(top_k_sampling) if top_k_sampling is not None else 2,
        (vlm_model_scale or "small").strip().lower() or "small",
        int(caption_batch_size) if caption_batch_size is not None else 4,
        (vlm_quantization or "none").strip().lower() or "none",
        int(max_image_size) if max_image_size is not None else 512,
        bool(force_rerun_step1_2), int(random_seed) if random_seed is not None else 1,
    )
    fallback_hint = get_modified_hint(*fallback)

    try:
        name = (config_name or "default_cfg").strip() or "default_cfg"
        project_root = Path(__file__).parent.parent
        target_path = project_root / "config" / f"{name}.yaml"
        if not target_path.exists() and name == "default_cfg":
            target_path = project_root / "config" / "config.yaml"
        if not target_path.exists():
            return fallback + (fallback_hint,), f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: config/{name}.yaml"

        loader = ConfigLoader(config_path=str(target_path))

        input_dir_raw = loader.get("data.input_directory", "test_pics") or "test_pics"
        try:
            inp = Path(input_dir_raw)
            if inp.is_absolute() and str(inp).startswith(str(project_root)):
                input_dir = str(inp.relative_to(project_root))
            else:
                input_dir = input_dir_raw
        except Exception:
            input_dir = input_dir_raw

        values = (
            input_dir,
            loader.get("embedding.provider", "dinov2"),
            loader.get("embedding.device", "cuda") or loader.get("vlm.device", "cuda") or "cuda",
            loader.get("clustering.backend", "hdbscan"),
            int(loader.get("clustering.min_samples", 2)),
            float(loader.get("clustering.epsilon", 1.0)),
            loader.get("clustering.dbscan_metric", "euclidean"),
            loader.get("clustering.dbscan_algorithm", "auto"),
            loader.get("clustering.cluster_selection_method", "leaf"),
            float(loader.get("clustering.cluster_selection_epsilon", 0.0)),
            float(loader.get("clustering.cluster_selection_persistence", 0.4)),
            int(loader.get("embedding.batch_size", 16)),
            loader.get("postprocessing.caption_mode", "representative") or "representative",
            int(loader.get("postprocessing.top_k_sampling", 2)),
            (loader.get("vlm.model_scale", "small") or "small").strip().lower(),
            int(loader.get("vlm.caption_batch_size", 4)),
            (loader.get("vlm.quantization", "none") or "none").strip().lower(),
            int(loader.get("vlm.max_image_size", 512)),
            False,  # force_rerun_step1_2 ä¸ä¿å­˜åˆ°é…ç½®ï¼ŒåŠ è½½æ—¶é»˜è®¤ä¸å‹¾é€‰
            int(loader.get("system.seed", 1)),
        )
        hint = get_modified_hint(*values)
        return values + (hint,), f"âœ… å·²åŠ è½½ config/{name}.yaml"
    except Exception as e:
        return fallback + (fallback_hint,), f"âŒ è¯»å–å¤±è´¥: {str(e)}"


def get_modified_hint(
    input_dir,
    embedding_provider,
    run_device,
    cluster_backend,
    min_samples,
    epsilon,
    dbscan_metric,
    dbscan_algorithm,
    cluster_selection_method,
    cluster_selection_epsilon,
    cluster_selection_persistence,
    batch_size,
    caption_mode,
    top_k_sampling,
    vlm_model_scale,
    caption_batch_size,
    vlm_quantization,
    max_image_size,
    force_rerun_step1_2,
    random_seed,
) -> str:
    """è¿”å›ä¸€è¡Œæç¤ºï¼šå½“å‰å·²ä¿®æ”¹çš„å‚æ•°ï¼ˆæ©™è‰²æ ‡è®°ï¼‰"""
    values = {
        "input_dir": (input_dir or "").strip(),
        "embedding_provider": embedding_provider,
        "run_device": (run_device or "cuda").strip() or "cuda",
        "cluster_backend": cluster_backend,
        "min_samples": int(min_samples),
        "epsilon": float(epsilon),
        "dbscan_metric": dbscan_metric,
        "dbscan_algorithm": dbscan_algorithm,
        "cluster_selection_method": cluster_selection_method,
        "cluster_selection_epsilon": float(cluster_selection_epsilon),
        "cluster_selection_persistence": float(cluster_selection_persistence),
        "batch_size": int(batch_size),
        "caption_mode": (caption_mode or "representative").strip() or "representative",
        "top_k_sampling": int(top_k_sampling) if top_k_sampling is not None else 2,
        "vlm_model_scale": (vlm_model_scale or "small").strip().lower() or "small",
        "caption_batch_size": int(caption_batch_size) if caption_batch_size is not None else 4,
        "vlm_quantization": (vlm_quantization or "none").strip().lower() or "none",
        "max_image_size": int(max_image_size) if max_image_size is not None else 512,
    }
    modified = [UI_PARAM_LABELS[k] for k in UI_PARAM_NAMES if values[k] != UI_DEFAULTS[k]]
    if not modified:
        return "<span style='font-size:0.9em'><b>å½“å‰å·²ä¿®æ”¹:</b> æ— ï¼ˆå‡ä¸ºé»˜è®¤ï¼‰</span>"
    return "<span style='font-size:0.9em'><b>å½“å‰å·²ä¿®æ”¹:</b> <span style='color:#e65100;font-weight:bold'>" + "ã€".join(modified) + "</span></span>"


from core.step0_indexing import run_step0
from core.step1_embedding import run_step1
from core.step2_clustering import run_step2
from core.step3_sampling import run_step3
from core.step4_caption import run_step4
from core.step5_label import run_step5
from core.step8_organization import run_step8


class SemanticClusterApp:
    """
    Semantic Cluster Webåº”ç”¨
    """
    
    def __init__(self):
        """åˆå§‹åŒ–åº”ç”¨"""
        self.config_loader = ConfigLoader()
        project_root = Path(__file__).parent.parent
        self.output_base = project_root / "data" / "output"
        self.output_base.mkdir(parents=True, exist_ok=True)
    
    def run_pipeline(
        self,
        input_dir: str,
        embedding_provider: str,
        run_device: str,
        cluster_backend: str,
        min_samples: int,
        epsilon: float,
        dbscan_metric: str,
        dbscan_algorithm: str,
        cluster_selection_method: str,
        cluster_selection_epsilon: float,
        cluster_selection_persistence: float,
        batch_size: int,
        caption_mode: str,
        top_k_sampling: int,
        vlm_model_scale: str,
        caption_batch_size: int,
        vlm_quantization: str,
        max_image_size: int = 512,
        force_rerun_step1_2: bool = False,
        random_seed: int = 1,
    ):
        """
        è¿è¡Œå®Œæ•´æµç¨‹
        
        Args:
            input_dir: è¾“å…¥ç›®å½•
            cluster_backend: èšç±»ç®—æ³• hdbscan/dbscan
            min_samples: æœ€å°æ ·æœ¬æ•°
            epsilon: DBSCANè·ç¦»é˜ˆå€¼
            cluster_selection_method: HDBSCANç°‡é€‰æ‹©æ–¹æ³•
            batch_size: æ‰¹é‡å¤§å°
            
        Returns:
            ç»“æœæ¶ˆæ¯å’Œç»Ÿè®¡ä¿¡æ¯
        """
        try:
            # éªŒè¯è¾“å…¥ - æ”¯æŒç›¸å¯¹è·¯å¾„ï¼ˆå¦‚test_picsï¼‰ä»é¡¹ç›®æ ¹è§£æ
            input_path = Path(input_dir.strip())
            if not input_path.is_absolute():
                project_root = Path(__file__).parent.parent
                input_path = project_root / input_path
            if not input_dir or not input_path.exists():
                yield 0, f"âŒ é”™è¯¯: è¾“å…¥ç›®å½•ä¸å­˜åœ¨ï¼({input_path})", "", ""
                return
            
            # åˆ›å»ºä¼šè¯è¾“å‡ºç›®å½•
            session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = self.output_base / session_id
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # æ›´æ–°é…ç½®
            self.config_loader.set("data.input_directory", str(input_path))
            self.config_loader.set("embedding.provider", embedding_provider)
            self.config_loader.set("embedding.device", run_device)
            backbone = "dinov2_vitl14" if embedding_provider == "dinov2" else "clip_vitb16"
            self.config_loader.set("embedding.backbone", backbone)
            self.config_loader.set("clustering.backend", cluster_backend)
            self.config_loader.set("clustering.min_samples", min_samples)
            self.config_loader.set("clustering.epsilon", epsilon)
            self.config_loader.set("clustering.dbscan_metric", dbscan_metric)
            self.config_loader.set("clustering.dbscan_algorithm", dbscan_algorithm)
            self.config_loader.set("clustering.cluster_selection_method", cluster_selection_method)
            self.config_loader.set("clustering.min_cluster_size", None)
            self.config_loader.set("clustering.cluster_selection_epsilon", cluster_selection_epsilon)
            self.config_loader.set("clustering.cluster_selection_persistence", cluster_selection_persistence)
            self.config_loader.set("embedding.batch_size", batch_size)
            self.config_loader.set("postprocessing.caption_mode", (caption_mode or "representative").strip() or "representative")
            self.config_loader.set("postprocessing.top_k_sampling", int(top_k_sampling) if top_k_sampling is not None else 2)
            self.config_loader.set("vlm.model_scale", (vlm_model_scale or "small").strip().lower() or "small")
            self.config_loader.set("vlm.caption_batch_size", int(caption_batch_size) if caption_batch_size is not None else 4)
            self.config_loader.set("vlm.quantization", (vlm_quantization or "none").strip().lower() or "none")
            self.config_loader.set("vlm.max_image_size", int(max_image_size) if max_image_size is not None else 512)
            self.config_loader.set("vlm.device", (run_device or "cuda").strip() or "cuda")
            seed_val = int(random_seed) if random_seed is not None else 1
            self.config_loader.set("system.seed", seed_val if seed_val >= 0 else -1)
            config = self.config_loader.to_dict()
            
            # æ§åˆ¶éšæœºæ€§ï¼šseed>=0 æ—¶è®¾ç½®å…¨å±€ç§å­ï¼Œ-1 åˆ™éšæœº
            if seed_val >= 0:
                import numpy as np
                np.random.seed(seed_val)
                try:
                    import torch
                    torch.manual_seed(seed_val)
                    if torch.cuda.is_available():
                        torch.cuda.manual_seed_all(seed_val)
                except ImportError:
                    pass
            
            # ä¿å­˜å®Œæ•´é…ç½®åˆ°è¾“å‡ºç›®å½•
            config_path = output_dir / "run_config.yaml"
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
            results = []
            stats_summary = []
            cluster_info = []
            current_step = 0  # 0=ç­‰å¾…, 1=Step-0, 2=Step-1, 3=Step-2, 4=å®Œæˆ
            
            def _yield():
                return current_step, "\n".join(results), "\n".join(stats_summary), "\n".join(cluster_info)
            
            def _run_captured(fn, *args, **kwargs):
                buf = io.StringIO()
                with redirect_stdout(buf):
                    return fn(*args, **kwargs), buf.getvalue().strip()
            
            # Step-0: ç´¢å¼•
            current_step = 1
            results.append("=" * 60)
            results.append("Step-0: å›¾åƒç´¢å¼•")
            results.append("=" * 60)
            results.append(f"ğŸ“‹ é…ç½®å·²ä¿å­˜: {config_path.name}")
            if seed_val >= 0:
                results.append(f"ğŸ² éšæœºç§å­: {seed_val} (å¯å¤ç°)")
            else:
                results.append(f"ğŸ² éšæœºç§å­: -1 (éšæœº)")
            results.append("")
            yield _yield()
            
            try:
                index_path, log_out = _run_captured(run_step0, config, output_dir)
                if log_out:
                    results.append(log_out)
                with open(index_path, 'r', encoding='utf-8') as f:
                    index = json.load(f)
                results.append(f"âœ… ç´¢å¼•å®Œæˆ: {len(index)} å¼ å›¾åƒ")
                stats_summary.append(f"æ€»å›¾åƒæ•°: {len(index)}")
            except Exception as e:
                results.append(f"âŒ ç´¢å¼•å¤±è´¥: {str(e)}")
                yield _yield()
                return
            yield _yield()

            # æ£€æŸ¥ A/B æœªæ”¹æ—¶æ˜¯å¦å¯å¤ç”¨åµŒå…¥ç¼“å­˜
            project_root = Path(__file__).parent.parent
            cache_base = project_root / "data" / ".cache" / "embedding"
            cache_key = _embed_cache_key(input_path, index, embedding_provider, run_device)
            cache_dir = cache_base / cache_key
            features_path = output_dir / "S1_embeddings.npy"
            image_ids_path = output_dir / "S1_image_ids.json"
            stats_path = output_dir / "S1_stats.json"
            cache_hit = (
                not force_rerun_step1_2
                and (cache_dir / "S1_embeddings.npy").exists()
                and (cache_dir / "S1_image_ids.json").exists()
                and (cache_dir / "S1_paths.json").exists()  # éœ€è¦è·¯å¾„æ˜ å°„æ‰èƒ½æ­£ç¡® remap
            )

            # Step-1: åµŒå…¥ï¼ˆç¼“å­˜å‘½ä¸­åˆ™è·³è¿‡ï¼‰
            current_step = 2
            results.append("\n" + "=" * 60)
            results.append("Step-1: ç‰¹å¾åµŒå…¥")
            results.append("=" * 60)
            if cache_hit:
                results.append("â™»ï¸ A/B æœªæ”¹ï¼Œå¤ç”¨ä¸Šæ¬¡åµŒå…¥ç»“æœ...")
                yield _yield()
                try:
                    # æŒ‰è·¯å¾„ remapï¼Œé¿å…é‡å¯åæ‰«æé¡ºåºå˜åŒ–å¯¼è‡´ç´¢å¼•é”™ä½
                    cached_embeddings = np.load(cache_dir / "S1_embeddings.npy")
                    with open(cache_dir / "S1_paths.json", 'r', encoding='utf-8') as f:
                        cached_paths = json.load(f)
                    path_to_embedding = {p: cached_embeddings[i] for i, p in enumerate(cached_paths)}
                    # æ£€æŸ¥å½“å‰ index çš„è·¯å¾„æ˜¯å¦éƒ½åœ¨ç¼“å­˜ä¸­
                    current_paths = {index[k]["path"] for k in index}
                    if current_paths != set(path_to_embedding.keys()):
                        raise ValueError(
                            "ç¼“å­˜è·¯å¾„ä¸å½“å‰ç´¢å¼•ä¸ä¸€è‡´ï¼ˆå›¾åƒå¯èƒ½å·²å˜æ›´ï¼‰ï¼Œå°†é‡æ–°è®¡ç®—åµŒå…¥"
                        )
                    # æŒ‰å½“å‰ index è·¯å¾„æ’åºï¼Œä¿è¯ä¸ S0_image_index.json ä¸€è‡´
                    sorted_ids = sorted(index.keys(), key=lambda k: index[k]["path"])
                    new_features = np.array([path_to_embedding[index[k]["path"]] for k in sorted_ids])
                    new_image_ids = sorted_ids
                    np.save(features_path, new_features)
                    with open(image_ids_path, 'w', encoding='utf-8') as f:
                        json.dump(new_image_ids, f, indent=2)
                    if (cache_dir / "S1_stats.json").exists():
                        shutil.copy2(cache_dir / "S1_stats.json", stats_path)
                    else:
                        stats = {'feature_dim': new_features.shape[1]}
                        with open(stats_path, 'w', encoding='utf-8') as f:
                            json.dump(stats, f, indent=2)
                    with open(stats_path, 'r') as f:
                        stats = json.load(f)
                    results.append(f"âœ… å¤ç”¨å®Œæˆ | ç»´åº¦: {stats['feature_dim']}")
                    stats_summary.append(f"ç‰¹å¾ç»´åº¦: {stats['feature_dim']}")
                except Exception as e:
                    results.append(f"âš ï¸ å¤ç”¨å¤±è´¥ï¼Œé‡æ–°è®¡ç®—: {e}")
                    cache_hit = False
            if not cache_hit:
                results.append("[1/3] åŠ è½½è§†è§‰æ¨¡å‹...")
                yield _yield()
                results.append("[2/3] æå–ç‰¹å¾ä¸­ï¼ˆè¯·ç¨å€™ï¼‰...")
                yield _yield()
                try:
                    step1_queue = Queue()
                    result_holder = [None]

                    def step1_progress(batch_idx, total_batches, n_done, n_total):
                        step1_queue.put(("progress", batch_idx, total_batches, n_done, n_total))

                    def step1_thread():
                        try:
                            buf = io.StringIO()
                            with redirect_stdout(buf):
                                path = run_step1(config, index_path, output_dir, progress_callback=step1_progress)
                            result_holder[0] = (path, buf.getvalue().strip(), None)
                        except Exception as e:
                            result_holder[0] = (None, None, e)
                        step1_queue.put(("done",))

                    t = Thread(target=step1_thread)
                    t.start()

                    while t.is_alive():
                        try:
                            msg = step1_queue.get(timeout=0.3)
                            if msg[0] == "done":
                                break
                            _, batch_idx, total, n_done, n_total = msg
                            results[-1] = f"[2/3] å·²å¤„ç† {n_done}/{n_total} å¼ å›¾åƒ (batch {batch_idx}/{total})"
                            yield _yield()
                        except Empty:
                            yield _yield()
                    t.join()

                    features_path, log_out, err = result_holder[0]
                    if err is not None:
                        raise err
                    results[-1] = "[2/3] æå–ç‰¹å¾å®Œæˆ"
                    if log_out:
                        results.append("")
                        results.append(log_out)
                    stats_path = output_dir / "S1_stats.json"
                    with open(stats_path, 'r') as f:
                        stats = json.load(f)
                    results.append(f"âœ… ç‰¹å¾æå–å®Œæˆ | ç»´åº¦: {stats['feature_dim']}")
                    stats_summary.append(f"ç‰¹å¾ç»´åº¦: {stats['feature_dim']}")
                    # å†™å…¥åµŒå…¥ç¼“å­˜ä¾›ä¸‹æ¬¡ A/B æœªæ”¹æ—¶å¤ç”¨ï¼ˆå« S1_paths.json ä¾› remapï¼‰
                    cache_dir.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(output_dir / "S1_embeddings.npy", cache_dir / "S1_embeddings.npy")
                    shutil.copy2(output_dir / "S1_image_ids.json", cache_dir / "S1_image_ids.json")
                    shutil.copy2(output_dir / "S1_stats.json", cache_dir / "S1_stats.json")
                    paths_path = output_dir / "S1_paths.json"
                    if paths_path.exists():
                        shutil.copy2(paths_path, cache_dir / "S1_paths.json")
                except Exception as e:
                    results.append(f"âŒ ç‰¹å¾æå–å¤±è´¥: {str(e)}")
                    yield _yield()
                    return
            yield _yield()
            
            # Step-2: èšç±»
            current_step = 3
            results.append("\n" + "=" * 60)
            results.append("Step-2: èšç±»")
            results.append("=" * 60)
            
            try:
                clustering_path, log_out = _run_captured(
                    run_step2,
                    config, features_path,
                    output_dir / "S1_image_ids.json",
                    output_dir
                )
                if log_out:
                    results.append(log_out)
                stats_path = output_dir / "S2_stats.json"
                with open(stats_path, 'r') as f:
                    stats = json.load(f)
                results.append(f"âœ… èšç±»å®Œæˆ")
                results.append(f"   ç°‡æ•°é‡: {stats['n_clusters']} | å™ªéŸ³: {stats['n_noise']} ({stats['noise_ratio']:.1f}%)")
                stats_summary.append(f"ç°‡æ•°é‡: {stats['n_clusters']}")
                stats_summary.append(f"å™ªéŸ³å›¾åƒ: {stats['n_noise']} ({stats['noise_ratio']:.1f}%)")
                cluster_info = [f"ç°‡ {cid}: {size}å¼ " for cid, size in stats['cluster_sizes'].items()]
            except Exception as e:
                results.append(f"âŒ èšç±»å¤±è´¥: {str(e)}")
                yield _yield()
                return
            yield _yield()

            # Phase-3: Step-3 å¤šç‚¹é‡‡æ · â†’ Step-4 å¹¶è¡Œæè¿° â†’ Step-5 è¯­ä¹‰è’¸é¦ï¼ˆD2=è·³è¿‡ æ—¶çœç•¥ï¼Œç”¨ç°‡åºå·å‘½åï¼‰
            labels_path = None
            vlm_scale_val = (vlm_model_scale or "small").strip().lower()
            if vlm_scale_val == "skip":
                results.append("\n" + "=" * 60)
                results.append("è·³è¿‡ Step-3/4/5ï¼ˆD2=è·³è¿‡ï¼Œä½¿ç”¨ç°‡åºå·ä½œä¸ºæ ‡ç­¾ï¼‰")
                results.append("=" * 60)
                yield _yield()
            else:
                try:
                    results.append("\n" + "=" * 60)
                    results.append("Step-3: å¤šç‚¹é‡‡æ ·")
                    results.append("=" * 60)
                    sampled_path, log_out = _run_captured(
                        run_step3,
                        config,
                        output_dir / "S1_embeddings.npy",
                        output_dir / "S1_image_ids.json",
                        clustering_path,
                        output_dir,
                    )
                    if log_out:
                        results.append(log_out)
                    results.append("âœ… é‡‡æ ·å®Œæˆ")
                    yield _yield()

                    results.append("\n" + "=" * 60)
                    results.append("Step-4: å¹¶è¡Œæè¿°")
                    results.append("=" * 60)
                    caption_mode_val = (caption_mode or "representative").strip() or "representative"
                    self.config_loader.set("postprocessing.caption_mode", caption_mode_val)
                    config = self.config_loader.to_dict()
                    results.append("Step-4: æ­£åœ¨æè¿°â€¦ 0/?")
                    yield _yield()
                    step4_queue = Queue()
                    step4_holder = [None]

                    def step4_progress(n: int, total: int, _iid: str):
                        step4_queue.put(("progress", n, total))

                    def step4_thread():
                        try:
                            buf = io.StringIO()
                            with redirect_stdout(buf):
                                path = run_step4(
                                    config,
                                    index_path,
                                    clustering_path,
                                    output_dir,
                                    mode=caption_mode_val,
                                    sampled_path=sampled_path,
                                    progress_callback=step4_progress,
                                )
                            step4_holder[0] = (path, buf.getvalue().strip(), None)
                        except Exception as e:
                            step4_holder[0] = (None, None, e)
                        step4_queue.put(("done",))

                    t4 = Thread(target=step4_thread)
                    t4.start()
                    while t4.is_alive():
                        try:
                            msg = step4_queue.get(timeout=0.3)
                            if msg[0] == "done":
                                break
                            _, n, total = msg
                            pct = (100 * n // total) if total else 0
                            results[-1] = f"Step-4: å·²æè¿° {n}/{total} ({pct}%)"
                            yield _yield()
                        except Empty:
                            yield _yield()
                    t4.join()
                    _, log_out, err4 = step4_holder[0]
                    if err4 is not None:
                        raise err4
                    results[-1] = "Step-4: å¹¶è¡Œæè¿°å®Œæˆ"
                    if log_out:
                        results.append("")
                        results.append(log_out)
                    results.append("âœ… æè¿°å®Œæˆ")
                    yield _yield()

                    results.append("\n" + "=" * 60)
                    results.append("Step-5: è¯­ä¹‰è’¸é¦")
                    results.append("=" * 60)
                    labels_path, log_out = _run_captured(
                        run_step5,
                        config,
                        output_dir / "S4_captions.json",
                        sampled_path,
                        output_dir,
                    )
                    if log_out:
                        results.append(log_out)
                    results.append("âœ… ç°‡æ ‡ç­¾å®Œæˆ")
                    yield _yield()
                except Exception as e:
                    results.append(f"âš ï¸ Phase-3 æŸæ­¥å¤±è´¥ï¼ˆç»§ç»­ç”¨æ— æ ‡ç­¾æ•´ç†ï¼‰: {str(e)}")
                    labels_path = None
                    yield _yield()
            
            # Step-8: æ•´ç†ï¼ˆæœ‰ S5 æ—¶ä½¿ç”¨è¯­ä¹‰æ ‡ç­¾å‘½åï¼‰
            results.append("\n" + "=" * 60)
            results.append("Step-8: æ–‡ä»¶æ•´ç†")
            results.append("=" * 60)
            
            try:
                organized_dir = output_dir / "organized"
                _, log_out = _run_captured(
                    run_step8,
                    config, index_path, clustering_path, output_dir, organized_dir,
                    dry_run=False,
                    labels_path=labels_path,
                )
                if log_out:
                    results.append(log_out)
                results.append(f"âœ… æ–‡ä»¶æ•´ç†å®Œæˆ | è¾“å‡º: {organized_dir}")
            except Exception as e:
                results.append(f"âŒ æ–‡ä»¶æ•´ç†å¤±è´¥: {str(e)}")
                yield _yield()
                return
            yield _yield()
            
            # å®Œæˆ
            current_step = 4
            results.append("\n" + "=" * 60)
            results.append("ğŸ‰ æ‰€æœ‰æ­¥éª¤å®Œæˆ!")
            results.append("=" * 60)
            results.append(f"\nä¼šè¯ID: {session_id}")
            results.append(f"è¾“å‡ºç›®å½•: {output_dir}")
            yield _yield()
            
        except Exception as e:
            import traceback
            error_msg = f"âŒ å‘ç”Ÿé”™è¯¯:\n{str(e)}\n\n{traceback.format_exc()}"
            yield 0, error_msg, "", ""


def create_ui():
    """
    åˆ›å»ºGradioç•Œé¢
    """
    app = SemanticClusterApp()
    
    # L1=ä¸»èŠ‚ L2=å­èŠ‚ L3=å•å‚æ•° - æŒ‰çº§åˆ«è°ƒæ•´å­—ä½“ã€æ¡†å¤§å°ã€é—´éš”
    PARAM_BOX_CSS = """
    #config-column { font-size: 0.9rem; }
    #config-column .gr-form, #config-column .gr-box { min-height: auto !important; }

    /* L1: ä¸»èŠ‚ - æœ€å¤§å­—ä½“ã€å¤§æ¡†ã€çª„é—´éš” */
    #config-column .param-l1 { margin-bottom: 6px; }
    #config-column .param-l1 > .wrap { margin-bottom: 3px !important; }
    #config-column .param-l1 button.label-wrap {
        font-size: 0.98rem !important; font-weight: 600 !important;
        padding: 5px 8px !important; min-height: 32px !important;
        display: flex !important; flex-direction: row !important;
        justify-content: space-between !important; width: 100% !important;
        user-select: text !important; -webkit-user-select: text !important;
    }
    #config-column .param-l1 .gr-form { padding: 2px 0 2px 6px !important; }
    #config-column .param-l1 .param-l2 { margin-bottom: 4px !important; }

    /* L2: å­èŠ‚ - ä¸­å­—ä½“ã€ä¸­æ¡†ã€çª„é—´éš”ï¼Œå¤–æ¡†é»‘è¾¹ */
    #config-column .param-l2 {
        margin-bottom: 4px;
        border: 2px solid #0d1b4d;
        border-radius: 4px;
        background: rgba(13, 27, 77, 0.8);
        padding: 3px 4px;
    }
    #config-column .param-l2 > .wrap { margin-bottom: 2px !important; }
    #config-column .param-l2 button.label-wrap {
        font-size: 0.88rem !important; font-weight: 500 !important;
        padding: 4px 6px !important; min-height: 28px !important;
        display: flex !important; flex-direction: row !important;
        justify-content: space-between !important; width: 100% !important;
        user-select: text !important; -webkit-user-select: text !important;
    }
    #config-column .param-l2 .gr-form { padding: 1px 0 1px 4px !important; }
    #config-column .param-l2 .param-l3 { margin-bottom: 2px !important; }

    /* L3: å•å‚æ•° - å°å­—ä½“ã€ç´§å‡‘æ¡†ã€çª„é—´éš” */
    #config-column .param-l3 { margin-bottom: 2px; }
    #config-column .param-l3 .wrap, #config-column .param-l3 > div { margin-bottom: 0 !important; }
    #config-column .param-l3 label { font-size: 0.82rem !important; font-weight: 500 !important; }
    #config-column .param-l3 .gr-form, #config-column .param-l3 .gr-box {
        margin-bottom: 1px !important; min-height: auto !important; padding: 1px 0 !important;
    }
    #config-column .param-l3 input:not([type="range"]), #config-column .param-l3 select,
    #config-column .param-l3 textarea {
        font-size: 0.82rem !important; padding: 4px 6px !important; min-height: 28px !important;
    }
    #config-column .param-l3 .block-info { font-size: 0.74rem !important; }
    #config-column .param-l3 .wrap.slider { align-items: center !important; }
    #config-column .param-l3 input[type="range"] { min-height: unset !important; padding: 0 !important; }

    /* L3 å†…åµŒå¥—åœ¨ L2 ä¸­æ—¶æ›´ç´§å‡‘ */
    #config-column .param-l2 .param-l3 label { font-size: 0.8rem !important; }
    #config-column .param-l2 .param-l3 input:not([type="range"]), #config-column .param-l2 .param-l3 select,
    #config-column .param-l2 .param-l3 textarea {
        font-size: 0.8rem !important; padding: 3px 5px !important; min-height: 26px !important;
    }
    #config-column .param-l2 .param-l3 input[type="range"] { min-height: unset !important; padding: 0 !important; }
    """
    # ç”¨ gr.Button.click(js=...) åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œï¼ŒGradio Accordion ä¸º button.label-wrapï¼Œå±•å¼€æ—¶æœ‰ .open
    JS_COLLAPSE_ALL = """() => {
        var root = document.getElementById('config-column') || document.body;
        var toggles = root.querySelectorAll('button.label-wrap.open');
        if (!toggles.length) toggles = root.querySelectorAll('[aria-expanded="true"]');
        Array.from(toggles).forEach((el, i) => setTimeout(() => el.click(), i * 100));
    }"""
    JS_EXPAND_ALL = """() => {
        var root = document.getElementById('config-column') || document.body;
        var toggles = root.querySelectorAll('button.label-wrap:not(.open)');
        if (!toggles.length) toggles = root.querySelectorAll('[aria-expanded="false"]');
        Array.from(toggles).forEach((el, i) => setTimeout(() => el.click(), i * 100));
    }"""
    with gr.Blocks(title="Semantic Cluster WebUI", theme=gr.themes.Soft(), css=PARAM_BOX_CSS) as demo:
        gr.Markdown("""
        # ğŸ¨ Semantic Cluster WebUI
        
        ç´¢å¼•ï¼ˆindexingï¼‰ â†’ åµŒå…¥ï¼ˆembeddingï¼‰ â†’ èšç±»ï¼ˆclusteringï¼‰ â†’ æŒ‰ç°‡æ•´ç†ï¼šé‡‡æ ·ï¼ˆsamplingï¼‰/æè¿°ï¼ˆcaptioningï¼‰
        """)
        
        with gr.Row():
            with gr.Column(scale=1, elem_id="config-column"):
                gr.Markdown("### âš™ï¸ é…ç½®å‚æ•°")
                with gr.Row():
                    btn_collapse_all = gr.Button("æŠ˜å æ‰€æœ‰", size="sm", scale=0)
                    btn_expand_all = gr.Button("å±•å¼€æ‰€æœ‰", size="sm", scale=0)
                btn_collapse_all.click(None, None, None, js=JS_COLLAPSE_ALL)
                btn_expand_all.click(None, None, None, js=JS_EXPAND_ALL)
                with gr.Group(elem_classes=["param-box", "param-l3"]):
                    gpu_status = gr.Markdown(
                        value=f"**GPU çŠ¶æ€**: {get_gpu_status()}",
                        elem_id="gpu-status"
                    )
                with gr.Accordion("A. æ•°æ®æº", open=False, elem_classes=["param-l1"]):
                    with gr.Group(elem_classes=["param-box", "param-l3"]):
                        input_dir = gr.Textbox(
                            label="A1 è¾“å…¥ç›®å½• (é»˜è®¤: test_pics)",
                            placeholder="ä¾‹å¦‚: D:/images æˆ–ä½¿ç”¨test_picsæµ‹è¯•",
                            value="test_pics",
                            info="å¾…èšç±»çš„å›¾åƒæ‰€åœ¨ç›®å½•ï¼Œæ”¯æŒç›¸å¯¹è·¯å¾„ï¼ˆå¦‚ test_picsï¼‰"
                        )
                    with gr.Accordion("å›ºå®šé¡¹", open=False, elem_classes=["param-l2"]):
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="A2 æ”¯æŒæ ¼å¼", value="jpg, jpeg, png, webp, bmp, tiff", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="A3 æœ€å°æ–‡ä»¶å¤§å°", value="0ï¼ˆä¸è¿‡æ»¤ï¼‰", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="A4 æœ€å¤§æ–‡ä»¶å¤§å°", value="-1ï¼ˆæ— é™åˆ¶ï¼‰", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="A5 æ’é™¤æ–‡ä»¶å¤¹", value="ï¼ˆç©ºï¼‰", interactive=False)
                
                with gr.Accordion("B. åµŒå…¥", open=False, elem_classes=["param-l1"]):
                    with gr.Group(elem_classes=["param-box", "param-l3"]):
                        embedding_provider = gr.Dropdown(
                            choices=[("DINOv2 (æ¨è)", "dinov2"), ("CLIP", "clip")],
                            value="dinov2",
                            label="B1 ç‰¹å¾æ¨¡å‹ (é»˜è®¤: dinov2)",
                            info="DINOv2 è§†è§‰è¯­ä¹‰æ›´å¼ºã€èšç±»æ›´ç¨³ï¼›CLIP æ”¯æŒå›¾æ–‡å¯¹é½ï¼Œé€‚åˆæœ‰æ ‡ç­¾åœºæ™¯"
                        )
                    with gr.Group(elem_classes=["param-box", "param-l3"]):
                        batch_size = gr.Slider(
                            minimum=4,
                            maximum=64,
                            value=16,
                            step=4,
                            label="B4 æ‰¹é‡å¤§å° (é»˜è®¤: 16)",
                            info="è¶Šå¤§è¶Šå¿«ä½†å æ›´å¤šæ˜¾å­˜ï¼›æ˜¾å­˜ä¸è¶³æ—¶å¯é™åˆ° 4â€“8"
                        )
                    with gr.Accordion("å›ºå®šé¡¹", open=False, elem_classes=["param-l2"]):
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="B2 Backbone", value="dinov2_vitl14 / clip_vitb16ï¼ˆç”± B1 å†³å®šï¼‰", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="B5 PCA Components", value="256", interactive=False)
                
                with gr.Accordion("C. èšç±»", open=False, elem_classes=["param-l1"]):
                    with gr.Accordion("é€šç”¨", open=False, elem_classes=["param-l2"]):
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            cluster_backend = gr.Dropdown(
                                choices=[("HDBSCAN (é»˜è®¤)", "hdbscan"), ("DBSCAN", "sklearn")],
                                value="hdbscan",
                                label="C1 èšç±»ç®—æ³• (é»˜è®¤: HDBSCAN)",
                                info="HDBSCAN è‡ªåŠ¨å‘ç°ç°‡æ•°é‡ã€å™ªéŸ³å°‘ï¼›DBSCAN éœ€æ‰‹åŠ¨è°ƒ Epsilonï¼Œç°‡æ•°æ›´å¯æ§"
                            )
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            min_samples = gr.Slider(
                                minimum=2,
                                maximum=30,
                                value=2,
                                step=1,
                                label="C6 Min Samples (é»˜è®¤: 2)",
                                info="è¶Šé«˜ç°‡è¶Šã€Œç´§å¯†ã€ã€æ•°é‡è¶Šå°‘ã€å™ªéŸ³è¶Šå¤šï¼›è¶Šä½ç°‡è¶Šå¤šã€å™ªéŸ³è¶Šå°‘"
                            )
                    with gr.Accordion("HDBSCAN", open=False, elem_classes=["param-l2"]):
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            cluster_selection_method = gr.Dropdown(
                                choices=[("leaf - å™ªéŸ³å°‘", "leaf"), ("eom - ç°‡å°‘", "eom")],
                                value="leaf",
                                label="C7 ç°‡é€‰æ‹©æ–¹æ³• (é»˜è®¤: leaf)",
                                info="leaf ç»†ç²’åº¦ã€ç°‡å¤šå™ªéŸ³å°‘ï¼›eom ä¿å®ˆã€ç°‡å°‘ä½†å™ªéŸ³æ•°å¾€å¾€æ›´å¤š"
                            )
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            cluster_selection_epsilon = gr.Slider(
                                minimum=0.0,
                                maximum=0.5,
                                value=0.0,
                                step=0.05,
                                label="C9 Cluster Sel Epsilon (é»˜è®¤: 0.0)",
                                info="ä¸¤ç°‡è·ç¦»å°äºæ­¤å€¼ä¼šåˆå¹¶ï¼›è¶Šå¤§ç°‡è¶Šå°‘ï¼›0-0.5 èŒƒå›´"
                            )
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            cluster_selection_persistence = gr.Slider(
                                minimum=0.0,
                                maximum=1.0,
                                value=0.4,
                                step=0.1,
                                label="C10 Cluster Sel Persistence (é»˜è®¤: 0.4)",
                                info="ç°‡åœ¨å±‚æ¬¡æ ‘ä¸­çš„å­˜æ´»é•¿åº¦é˜ˆå€¼ï¼šè¶Šé«˜ç°‡è¶Šå°‘ï¼›å»ºè®® 0.2-0.4"
                            )
                    with gr.Accordion("DBSCAN", open=False, elem_classes=["param-l2"]):
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            epsilon = gr.Slider(
                                minimum=0.5,
                                maximum=1.5,
                                value=1.0,
                                step=0.05,
                                label="C4 Epsilon (é»˜è®¤: 1.0)",
                                info="DBSCAN é‚»åŸŸåŠå¾„ï¼šè¶Šå¤§ç°‡è¶Šå°‘è¶Šå¤§ï¼›è¶Šå°ç°‡è¶Šå¤šè¶Šå°ï¼Œå™ªéŸ³å¯èƒ½å¢å¤š"
                            )
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            dbscan_metric = gr.Dropdown(
                                choices=[("euclideanï¼ˆæ¬§æ°ï¼‰", "euclidean"), ("cosineï¼ˆä½™å¼¦ï¼‰", "cosine")],
                                value="euclidean",
                                label="C4b è·ç¦»åº¦é‡ (é»˜è®¤: euclidean)",
                                info="L2 å½’ä¸€åŒ–ç‰¹å¾å¯è¯• cosineï¼›euclidean é€šç”¨"
                            )
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            dbscan_algorithm = gr.Dropdown(
                                choices=[("autoï¼ˆè‡ªåŠ¨ï¼‰", "auto"), ("ball_tree", "ball_tree"), ("kd_tree", "kd_tree"), ("bruteï¼ˆæš´åŠ›ï¼‰", "brute")],
                                value="auto",
                                label="C4c æœ€è¿‘é‚»ç®—æ³• (é»˜è®¤: auto)",
                                info="å½±å“é€Ÿåº¦ï¼šå¤§æ•°æ®é›†å¯è¯• ball_tree/kd_tree"
                            )
                    with gr.Accordion("å›ºå®šé¡¹", open=False, elem_classes=["param-l2"]):
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="C2 è·ç¦»åº¦é‡", value="euclidean", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="C3 èšç±»æ¨¡å¼", value="fixed_eps", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="C5 æœ€å¤§å™ªéŸ³æ¯”ä¾‹", value="20 %", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="C11 Alpha", value="1.0", interactive=False)
                
                with gr.Accordion("D. VLM", open=False, elem_classes=["param-l1"]):
                    with gr.Group(elem_classes=["param-box", "param-l3"]):
                        caption_mode = gr.Dropdown(
                            choices=[
                                ("ä»£è¡¨å›¾ï¼ˆæ¨¡å¼1ï¼Œéœ€ Step-3 é‡‡æ ·ï¼‰", "representative"),
                                ("å…¨éƒ¨å›¾ï¼ˆæ¨¡å¼2ï¼Œè·³è¿‡ Step-3ï¼‰", "all"),
                            ],
                            value="representative",
                            label="D5 æè¿°æ¨¡å¼ (Step-4)",
                            info="æ¨¡å¼1 ä»…æè¿°ä»£è¡¨å›¾ï¼›æ¨¡å¼2 æè¿°å…¨éƒ¨å›¾åƒ"
                        )
                    with gr.Group(elem_classes=["param-box", "param-l3"]):
                        vlm_model_scale = gr.Dropdown(
                            choices=[
                                ("2B (å¿«ï¼Œé»˜è®¤)", "small"),
                                ("7B (å‡†)", "large"),
                                ("è·³è¿‡ï¼ˆç”¨ç°‡åºå·ï¼‰", "skip"),
                            ],
                            value="small",
                            label="D2 æ¨¡å‹è§„æ¨¡ (Step-4/5)",
                            info="å°/å¤§æ¨¡å‹ç”¨äºæè¿°ä¸æ ‡ç­¾ï¼›é€‰ã€Œè·³è¿‡ã€åˆ™è·³è¿‡ Step-3/4/5ï¼Œç›´æ¥ç”¨ç°‡åºå·å‘½å"
                        )
                    with gr.Group(elem_classes=["param-box", "param-l3"]):
                        caption_batch_size = gr.Slider(
                            minimum=1,
                            maximum=16,
                            value=4,
                            step=1,
                            label="D6 æè¿°æ‰¹é‡ (Caption Batch Size)",
                            info="Step-4 æ¯æ‰¹å›¾åƒæ•°ï¼Œé»˜è®¤ 4"
                        )
                    with gr.Group(elem_classes=["param-box", "param-l3"]):
                        max_image_size = gr.Number(
                            value=512,
                            minimum=0,
                            maximum=2048,
                            step=64,
                            label="D10 æœ€å¤§åˆ†è¾¨ç‡ (é•¿è¾¹åƒç´ )",
                            info="æè¿°å‰é•¿è¾¹ç¼©è‡³æ­¤åƒç´ ä»¥åŠ é€Ÿï¼›0=ä¸ç¼©å°ï¼Œé»˜è®¤ 512"
                        )
                    with gr.Group(elem_classes=["param-box", "param-l3"]):
                        top_k_sampling = gr.Slider(
                            minimum=1,
                            maximum=20,
                            value=2,
                            step=1,
                            label="D7 Top-K é‡‡æ · (åŸ E2ï¼Œæ¯ç°‡ä»£è¡¨å›¾æ•°)",
                            info="Step-3 æ¯ç°‡é‡‡æ ·å¼ æ•°ï¼Œé»˜è®¤ 2ï¼›ä»…ä»£è¡¨å›¾æ¨¡å¼æ—¶ç”Ÿæ•ˆ"
                        )
                    with gr.Group(elem_classes=["param-box", "param-l3"]):
                        vlm_quantization = gr.Dropdown(
                            choices=[
                                ("æ— ", "none"),
                                ("int8", "int8"),
                                ("int4", "int4"),
                            ],
                            value="none",
                            label="D9 é‡åŒ– (int8/int4)",
                            info="éœ€å®‰è£… bitsandbytesï¼Œä»… CUDAï¼›çœæ˜¾å­˜ã€å¯æé€Ÿ"
                        )
                    with gr.Accordion("å›ºå®šé¡¹", open=False, elem_classes=["param-l2"]):
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="D1 Provider", value="local_qwen2vl", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="D2 Model Name", value="ç”± D2 è§„æ¨¡å†³å®š 2B/7B", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="D4 API Key", value="ï¼ˆæœªä½¿ç”¨ï¼‰", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="D8 Sampling Strategy (åŸ E1)", value="nearest", interactive=False)
                
                with gr.Accordion("E. åå¤„ç†", open=False, elem_classes=["param-l1"]):
                    with gr.Accordion("å›ºå®šé¡¹", open=False, elem_classes=["param-l2"]):
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="E3/E4 Caption/Label Prompt", value="ï¼ˆè§ configï¼‰", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="E5/E6 Caption/Label Length", value="50 / 5-10", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="E7 Rescue Threshold", value="0.60", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="E8 Similarity Algorithm", value="cosine", interactive=False)
                
                with gr.Accordion("F. è¾“å‡º", open=False, elem_classes=["param-l1"]):
                    with gr.Accordion("å›ºå®šé¡¹", open=False, elem_classes=["param-l2"]):
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="F1 Dimensionality Reduction", value="umap", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="F2 File Naming Rule", value="id@label@originalï¼ˆç°‡åºå·/ç°‡åºå·@ç°‡æ ‡ç­¾@åŸåï¼‰", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="F3 æè¿° .txt åˆ° output", value="true", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="F4 æ¯å¥å…³é”®è¯ .txt", value="true", interactive=False)
                
                with gr.Accordion("G. ä¼˜åŒ–", open=False, elem_classes=["param-l1"]):
                    with gr.Group(elem_classes=["param-box", "param-l3"]):
                        run_device = gr.Dropdown(
                            choices=[("GPU (cuda)", "cuda"), ("CPU", "cpu")],
                            value="cuda",
                            label="G10 è¿è¡Œè®¾å¤‡ (åµŒå…¥+VLM)",
                            info="Step-1 ç‰¹å¾æå–ä¸ Step-4/5 å›¾åƒæè¿°/æ ‡ç­¾è’¸é¦å…±ç”¨ï¼›cuda / cpu"
                        )
                    with gr.Group(elem_classes=["param-box", "param-l3"]):
                        random_seed = gr.Number(
                            value=1,
                            minimum=-1,
                            maximum=2147483647,
                            step=1,
                            label="G8 éšæœºæ•°ç§å­ (é»˜è®¤: 1)",
                            info="-1 è¡¨ç¤ºæ¯æ¬¡éšæœºï¼›â‰¥0 è¡¨ç¤ºå›ºå®šç§å­ï¼Œç»“æœå¯å¤ç°"
                        )
                    with gr.Group(elem_classes=["param-box", "param-l3"]):
                        force_rerun_step1_2 = gr.Checkbox(
                            value=False,
                            label="G9 é‡æ–°è¿›è¡Œå‰2æ­¥ï¼ˆç´¢å¼•+åµŒå…¥ï¼‰",
                            info="å‹¾é€‰åå¼ºåˆ¶é‡æ–°æ‰§è¡Œ Step-0 å’Œ Step-1ï¼Œå³ä½¿ A/B æœªæ”¹å¯å¤ç”¨ç¼“å­˜ï¼›ä¸å‹¾é€‰åˆ™å¤ç”¨ä¸Šæ¬¡åµŒå…¥ç»“æœ"
                        )
                    with gr.Accordion("å›ºå®šé¡¹", open=False, elem_classes=["param-l2"]):
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="G1 Enable Acceleration", value="True", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="G2 Num Workers", value="4", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="G3 Thumbnail Cache", value="True", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="G4 Mixed Precision", value="True", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="G5 Model Compile", value="False", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="G6 Embedding Cache", value="True", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="G7 Prefetch Factor", value="2", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="G11 è¾“å‡ºæ ¹ç›®å½•", value="data/output", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="G12 ç¼“å­˜ç›®å½•", value="data/.cache", interactive=False)
                        with gr.Group(elem_classes=["param-box", "param-l3"]):
                            gr.Textbox(label="G13 æ—¥å¿—çº§åˆ«", value="INFO", interactive=False)
                pipeline_inputs = [
                    input_dir, embedding_provider, run_device, cluster_backend,
                    min_samples, epsilon, dbscan_metric, dbscan_algorithm,
                    cluster_selection_method, cluster_selection_epsilon, cluster_selection_persistence,
                    batch_size, caption_mode, top_k_sampling, vlm_model_scale, caption_batch_size, vlm_quantization, max_image_size, force_rerun_step1_2, random_seed,
                ]
                modified_hint = gr.HTML(
                    value="<span style='font-size:0.9em'><b>å½“å‰å·²ä¿®æ”¹:</b> æ— ï¼ˆå‡ä¸ºé»˜è®¤ï¼‰</span>",
                    elem_id="modified-hint",
                )

                with gr.Accordion("é…ç½®ä¿å­˜", open=False, elem_classes=["param-l1"]):
                    config_selector = gr.Dropdown(
                        choices=get_config_choices(),
                        value="default_cfg",
                        label="é…ç½®å",
                        allow_custom_value=True,
                        info="é€‰æ‹©æˆ–è¾“å…¥åç§°ï¼Œä¿å­˜ä¸º config/xxx.yaml",
                    )
                    with gr.Row():
                        btn_load = gr.Button("åŠ è½½é…ç½®", size="sm")
                        btn_reset = gr.Button("æ¢å¤é»˜è®¤", size="sm")
                        btn_save = gr.Button("ä¿å­˜é…ç½®", size="sm")
                    config_status = gr.Textbox(label=None, value="", interactive=False, show_label=False, lines=1)

                with gr.Row():
                    run_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary", size="lg")
                    btn_open_latest = gr.Button("ğŸ“‚ æ‰“å¼€æœ€è¿‘ç»“æœ", size="sm", variant="secondary")
                
                gr.Markdown("""
                ### ğŸ“‹ ä½¿ç”¨æ­¥éª¤
                
                1. **è¾“å…¥ç›®å½•**: é€‰æ‹©åŒ…å«å›¾åƒçš„æ–‡ä»¶å¤¹
                2. **è°ƒæ•´å‚æ•°**: ï¼ˆå¯é€‰ï¼‰è°ƒæ•´èšç±»å‚æ•°
                3. **å¼€å§‹å¤„ç†**: ç‚¹å‡»æŒ‰é’®è¿è¡Œå®Œæ•´æµç¨‹
                4. **æŸ¥çœ‹ç»“æœ**: åœ¨å³ä¾§æŸ¥çœ‹å¤„ç†è¿›åº¦å’Œç»“æœ
                """)
            
            with gr.Column(scale=2):
                with gr.Accordion("ç»Ÿè®¡ä¸ç°‡åˆ†å¸ƒ", open=True):
                    with gr.Row():
                        stats_output = gr.Textbox(label="æ€»ä½“ç»Ÿè®¡", lines=6, scale=1)
                        cluster_output = gr.Textbox(label="ç°‡å¤§å°åˆ†å¸ƒ", lines=6, scale=1)
                
                with gr.Group():
                    gr.Markdown("#### ğŸ“ˆ è¿›åº¦")
                    progress_bar = gr.Slider(
                        0, 4, value=0, step=1,
                        label="é˜¶æ®µ (0-4)",
                        interactive=False,
                        show_label=True
                    )
                
                with gr.Group():
                    gr.Markdown("#### ğŸ“‹ æ—¥å¿—")
                    log_output = gr.Textbox(
                        show_label=False,
                        lines=20,
                        max_lines=35,
                        placeholder="ç‚¹å‡»ã€Œå¼€å§‹å¤„ç†ã€åï¼Œæ—¥å¿—å°†åœ¨æ­¤å¤„å®æ—¶æ›´æ–°..."
                    )
        
        # ç»‘å®šäº‹ä»¶
        btn_open_latest.click(fn=open_latest_organized, outputs=[])
        run_btn.click(
            fn=app.run_pipeline,
            inputs=pipeline_inputs,
            outputs=[progress_bar, log_output, stats_output, cluster_output]
        )

        def _load_wrapper(config_name, *args):
            vals, msg = load_ui_config(config_name, *args)
            return list(vals) + [msg]

        btn_load.click(
            fn=_load_wrapper,
            inputs=[config_selector] + pipeline_inputs,
            outputs=pipeline_inputs + [modified_hint, config_status],
        )

        btn_reset.click(
            fn=reset_to_defaults,
            inputs=None,
            outputs=pipeline_inputs + [modified_hint],
        )

        btn_save.click(
            fn=save_ui_config,
            inputs=[config_selector] + pipeline_inputs,
            outputs=[config_status],
        )

        # å‚æ•°å˜æ›´æ—¶æ›´æ–°â€œå½“å‰å·²ä¿®æ”¹â€æç¤ºï¼ˆéé»˜è®¤é¡¹æ©™è‰²æ˜¾ç¤ºï¼‰
        for inp in pipeline_inputs:
            inp.change(fn=get_modified_hint, inputs=pipeline_inputs, outputs=[modified_hint])
        
        gr.Markdown("""
        ---
        ### ğŸ’¡ æç¤º
        
        - **è¾“å‡ºä½ç½®**: `data/output/{ä¼šè¯ID}/organized/`
        - **ä¼šè¯ä¿ç•™**: æ‰€æœ‰ä¸­é—´æ–‡ä»¶éƒ½ä¿å­˜åœ¨ä¼šè¯ç›®å½•ä¸­
        
        ### ğŸ“ è¾“å‡ºç»“æ„
        
        ```
        organized/
        â”œâ”€â”€ cluster_00/ (ç°‡0çš„å›¾åƒ)
        â”œâ”€â”€ cluster_01/ (ç°‡1çš„å›¾åƒ)
        â”œâ”€â”€ ...
        â””â”€â”€ noise/ (æœªåˆ†ç±»çš„å›¾åƒ)
        ```
        
        ---
        **Version**: Phase-1 MVP | **Date**: 2026-01-31
        """)
    
    return demo


if __name__ == "__main__":
    print("=" * 60)
    print("  Semantic Cluster WebUI - Phase 1")
    print("=" * 60)
    print("\næ­£åœ¨å¯åŠ¨Webç•Œé¢...")
    print("è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æ˜¾ç¤ºçš„URL\n")
    
    demo = create_ui()
    demo.queue().launch(  # queue() å¯ç”¨ç”Ÿæˆå™¨æµå¼è¾“å‡ºï¼Œå®ç°è¿›åº¦å’Œæ—¥å¿—å®æ—¶æ›´æ–°
        server_name="127.0.0.1",
        server_port=7860,
        share=False,
        show_error=True
    )
