"""
Step-4: å¹¶è¡Œæè¿°
ä½¿ç”¨ VLM å¯¹å›¾åƒç”Ÿæˆè¯­ä¹‰æè¿°ï¼Œè¾“å‡º S4_captions.jsonã€‚

æ¨¡å¼1: ä»…æè¿°ä»£è¡¨å›¾åƒï¼ˆéœ€ S3_sampled_images.jsonï¼‰
æ¨¡å¼2: æè¿°æ‰€æœ‰å›¾ç‰‡ï¼ˆè·³è¿‡ Step-3ï¼Œä» S2_clustering å–å…¨éƒ¨ image_idï¼‰

VLM æ¥æºï¼ˆconfig.vlmï¼‰:
- model_source: huggingfaceï¼ˆé»˜è®¤ï¼‰| modelscope
  ModelScope é€šä¹‰åƒé—®2-VL: https://www.modelscope.cn/models/qwen/Qwen2-VL-2B-Instruct/summary
  å›½å†…ä½¿ç”¨ model_source=modelscope å¯åŠ é€Ÿä¸‹è½½ï¼›éœ€ pip install modelscope
- model_scale: small(2B) | large(7B)ï¼Œæˆ– model_name æŒ‡å®šå…·ä½“ ID
- æœ¬åœ°åŠ è½½: transformers AutoModelForImageTextToText + AutoProcessor

ğŸ“… Last Updated: 2026-01-31
ğŸ“– Reference: docs/workflow-structure.md
"""

import json
import pandas as pd
from pathlib import Path
from typing import Callable, Dict, List, Optional

from tqdm import tqdm

ROOT = Path(__file__).parent.parent


def _load_image_list(
    mode: str,
    index_path: Path,
    clustering_path: Path,
    sampled_path: Optional[Path],
) -> List[str]:
    """æ ¹æ®æ¨¡å¼è¿”å›å¾…æè¿° image_id åˆ—è¡¨"""
    with open(index_path, "r", encoding="utf-8") as f:
        index = json.load(f)
    clustering = pd.read_csv(clustering_path)

    if mode == "representative" and sampled_path and sampled_path.exists():
        with open(sampled_path, "r", encoding="utf-8") as f:
            sampled = json.load(f)
        ids = []
        for cid, img_list in sampled.items():
            ids.extend(img_list)
        return list(dict.fromkeys(ids))

    # æ¨¡å¼2 æˆ– æ—  S3: æ‰€æœ‰å›¾åƒï¼ˆä¸å« noise å¯é€‰ï¼Œè¿™é‡Œå« noiseï¼‰
    return clustering["image_id"].astype(str).tolist()


def _caption_with_placeholder(image_ids: List[str], clustering_path: Path) -> Dict[str, str]:
    """æ—  VLM æ—¶ç”Ÿæˆå ä½æè¿°"""
    clustering = pd.read_csv(clustering_path)
    cid_map = dict(zip(clustering["image_id"].astype(str), clustering["cluster_id"]))
    return {
        iid: f"[Placeholder] Image in cluster {cid_map.get(iid, -1)}."
        for iid in image_ids
    }


def _caption_with_vlm(
    image_ids: List[str],
    index: Dict,
    config: dict,
    caption_prompt: str,
    device: str,
    progress_callback: Optional[Callable[[int, int, str], None]] = None,
) -> Dict[str, str]:
    """ä½¿ç”¨æœ¬åœ° VLMï¼ˆQwen2-VL ç­‰ï¼‰ç”Ÿæˆæè¿°ï¼›å¤±è´¥æˆ–æœªå®‰è£…æ—¶è¿”å›ç©º dict èµ°å ä½é€»è¾‘ã€‚"""
    import sys
    if str(ROOT) not in sys.path:
        sys.path.insert(0, str(ROOT))
    try:
        from models.vlm_models import (
            get_vlm_model_and_processor,
            caption_batch,
            resolve_vlm_model_name,
            is_vlm_available,
        )
    except ImportError as e:
        print(f"[Step-4] å¯¼å…¥ VLM æ¨¡å—å¤±è´¥: {e}")
        return {}
    if not is_vlm_available():
        return {}
    vlm_cfg = config.get("vlm", {})
    model_name = resolve_vlm_model_name(config)
    torch_dtype = vlm_cfg.get("torch_dtype", "bfloat16")
    use_flash_attn = vlm_cfg.get("use_flash_attn", True)
    quantization = (vlm_cfg.get("quantization") or "none").strip().lower()
    batch_size = max(1, int(vlm_cfg.get("caption_batch_size", 4)))
    max_image_size = max(0, int(vlm_cfg.get("max_image_size", 512)))
    max_new_tokens = min(512, max(64, int(config.get("postprocessing", {}).get("caption_length", 50)) * 3))
    if max_image_size > 0:
        print(f"[Step-4] æè¿°å‰ç¼©å°å›¾åƒï¼šé•¿è¾¹ â‰¤ {max_image_size} pxï¼ˆåŠ é€Ÿï¼‰")
    print(f"[Step-4] æ­£åœ¨åŠ è½½ VLM æ¨¡å‹ï¼ˆé¦–æ¬¡å¯èƒ½è¾ƒæ…¢ï¼‰: {model_name}")
    try:
        model, processor = get_vlm_model_and_processor(
            model_name,
            device=device,
            torch_dtype=torch_dtype,
            use_flash_attn=use_flash_attn,
            quantization=quantization if quantization not in ("", "none") else None,
            config=config,
        )
    except Exception as e:
        print(f"[Step-4] VLM åŠ è½½å¤±è´¥: {e}")
        return {}
    base_dir = Path(config.get("data", {}).get("input_directory", "."))
    items: List[tuple] = []
    for iid in image_ids:
        if iid not in index:
            continue
        path = Path(index[iid]["path"])
        if not path.is_absolute():
            path = (base_dir / path).resolve()
        if path.exists():
            items.append((iid, path))
    total = len(image_ids)
    captions: Dict[str, str] = {iid: "" for iid in image_ids}
    if not items:
        return captions
    print(f"[Step-4] æ¨¡å‹å·²åŠ è½½ï¼Œå¼€å§‹æ‰¹é‡æè¿°ï¼ˆå…± {len(items)} å¼ ï¼Œbatch_size={batch_size}ï¼‰")
    pbar = tqdm(
        total=len(items),
        desc="Step-4 æ‰¹é‡æè¿°",
        unit="å¼ ",
        bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} ({percentage:3.0f}%) [å·²ç”¨ {elapsed}, å‰©ä½™ {remaining}]",
    )
    done = 0
    total_batches = (len(items) + batch_size - 1) // batch_size
    for batch_idx, start in enumerate(range(0, len(items), batch_size)):
        chunk = items[start : start + batch_size]
        if batch_idx == 0 or (batch_idx + 1) % max(1, total_batches // 5) == 0 or batch_idx == total_batches - 1:
            print(f"[Step-4] æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}ï¼Œæœ¬æ‰¹ {len(chunk)} å¼ ")
        batch_result = caption_batch(
            model, processor, chunk, caption_prompt,
            max_new_tokens=max_new_tokens, batch_size=len(chunk),
            max_image_size=max_image_size,
        )
        for iid, cap in batch_result.items():
            captions[iid] = cap or ""
        done += len(chunk)
        if progress_callback:
            progress_callback(min(done, total), total, chunk[-1][0] if chunk else "")
        pbar.update(len(chunk))
        log_interval = max(1, total // 10)
        if done % log_interval < len(chunk) or done == len(items):
            print(f"[Step-4] è¿›åº¦ {done}/{len(items)} ({100 * done // len(items)}%)")
    pbar.close()
    return captions if any(captions.values()) else {}


def run_step4(
    config: dict,
    index_path: Path,
    clustering_path: Path,
    output_dir: Path,
    mode: str = "representative",
    sampled_path: Optional[Path] = None,
    progress_callback: Optional[Callable[[int, int, str], None]] = None,
) -> Path:
    """
    è¿è¡Œ Step-4: å¹¶è¡Œæè¿°

    Args:
        config: é…ç½®å­—å…¸
        index_path: S0_image_index.json
        clustering_path: S2_clustering.csv
        output_dir: è¾“å‡ºç›®å½•
        mode: "representative" ç”¨ S3 ä»£è¡¨å›¾ï¼Œ"all" ç”¨å…¨éƒ¨å›¾
        sampled_path: S3_sampled_images.jsonï¼ˆæ¨¡å¼1 å¿…é€‰ï¼‰

    Returns:
        S4_captions.json è·¯å¾„
    """
    print("=" * 60)
    print("Step-4: å¹¶è¡Œæè¿°")
    print("=" * 60)

    post = config.get("postprocessing", {})
    caption_tpl = post.get("caption_prompt", "Describe this image in about {caption_length} words.")
    caption_len = int(post.get("caption_length", 50))
    caption_prompt = caption_tpl.format(caption_length=caption_len)

    image_ids = _load_image_list(mode, index_path, clustering_path, sampled_path)
    total_images = len(image_ids)
    device = (config.get("vlm", {}).get("device") or "cuda").strip().lower() or "cuda"
    try:
        import torch
        if device == "cpu" and torch.cuda.is_available():
            device = "cuda"
            print("[Step-4] æ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ cudaï¼ˆconfig ä¸­ device å·²è¦†ç›–ï¼‰")
    except Exception:
        pass
    try:
        from models.vlm_models import resolve_vlm_model_name
        model_name = resolve_vlm_model_name(config)
    except ImportError:
        model_name = config.get("vlm", {}).get("model_name", "Qwen/Qwen2-VL-2B-Instruct")
    batch_size = max(1, int(config.get("vlm", {}).get("caption_batch_size", 4)))
    print(f"[Step-4] æ¨¡å¼: {mode}ï¼Œå¾…æè¿°: {total_images} å¼ ")
    print(f"[Step-4] æ¨¡å‹: {model_name}ï¼Œè®¾å¤‡: {device}ï¼Œæ‰¹é‡: {batch_size}ï¼Œæè¿°é•¿åº¦çº¦ {caption_len} è¯")

    # ç«‹å³é€šçŸ¥ UI æ€»æ•°ï¼Œé¿å…é•¿æ—¶é—´æ˜¾ç¤º 0/?
    if progress_callback and total_images > 0:
        progress_callback(0, total_images, "")

    # å…ˆæ£€æµ‹æ¨¡å‹å¯ç”¨å¹¶ä¸”å·²ä¸‹è½½
    import sys
    if str(ROOT) not in sys.path:
        sys.path.insert(0, str(ROOT))
    try:
        from models.vlm_models import check_vlm_ready
    except ImportError:
        check_vlm_ready = None
    vlm_ready = False
    if check_vlm_ready is not None:
        vlm_ready, vlm_msg = check_vlm_ready(config)
        if vlm_ready:
            print(f"[Step-4] {vlm_msg}")
        else:
            print(f"[Step-4] VLM æœªå°±ç»ªï¼š{vlm_msg}")

    with open(index_path, "r", encoding="utf-8") as f:
        index = json.load(f)

    if vlm_ready:
        captions = _caption_with_vlm(
            image_ids, index, config, caption_prompt, device,
            progress_callback=progress_callback,
        )
        if not captions:
            captions = _caption_with_placeholder(image_ids, clustering_path)
            print("[Step-4] VLM æ¨ç†æœªè¿”å›ç»“æœï¼Œä½¿ç”¨å ä½æè¿°ã€‚")
    else:
        captions = _caption_with_placeholder(image_ids, clustering_path)
        print("[Step-4] ä½¿ç”¨å ä½æè¿°ï¼ˆVLM æœªå°±ç»ªï¼‰ã€‚")

    out_path = output_dir / "S4_captions.json"
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(captions, f, indent=2, ensure_ascii=False)

    n_ok = sum(1 for v in captions.values() if (v or "").strip())
    print(f"[Step-4] å®Œæˆ: å…± {len(captions)} æ¡ï¼Œæœ‰æ•ˆæè¿° {n_ok} æ¡ -> {out_path.name}")

    # å°†å®Œæ•´æè¿° .txt è¾“å‡ºåˆ° output ç›®å½•ï¼ˆconfig.output.write_caption_txt ä¸º true æ—¶ï¼‰
    write_txt = config.get("output", {}).get("write_caption_txt", True)
    if write_txt and captions:
        txt_dir = output_dir / "caption_txt"
        txt_dir.mkdir(parents=True, exist_ok=True)
        txt_count = 0
        for iid, text in captions.items():
            if not (text or "").strip():
                continue
            txt_path = txt_dir / f"{iid}.txt"
            try:
                txt_path.write_text(text.strip(), encoding="utf-8")
                txt_count += 1
            except Exception as e:
                print(f"[Step-4] å†™å…¥ {txt_path} å¤±è´¥: {e}")
        if txt_count:
            print(f"[Step-4] å·²è¾“å‡º {txt_count} ä¸ªæè¿° .txt -> {txt_dir.relative_to(output_dir)}/")

    print("=" * 60)
    return out_path
