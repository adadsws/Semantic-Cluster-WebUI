"""
Step-5: è¯­ä¹‰è’¸é¦
æ ¹æ® S4 æè¿°ä¸ºæ¯ä¸ªç°‡ç”Ÿæˆè¯­ä¹‰æ ‡ç­¾ï¼Œè¾“å‡º S5_cluster_labels.csv

è¾“å…¥: S4_captions.json, S3_sampled_images.jsonï¼ˆç°‡â†’ä»£è¡¨å›¾IDï¼‰
è¾“å‡º: S5_cluster_labels.csv (cluster_id, label)
æ— çœŸå® LLM æ—¶ï¼šç”¨é¦–æ¡æè¿°æˆªæ–­æˆ–å ä½ "Cluster_N"ï¼Œå¹¶åšæ–‡ä»¶åå®‰å…¨åŒ–

ğŸ“… Last Updated: 2026-01-31
ğŸ“– Reference: docs/workflow-structure.md
"""

import json
import re
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional

ROOT = Path(__file__).parent.parent


def _sanitize_label(label: str, max_length: int = 512) -> str:
    """æ›¿æ¢æ–‡ä»¶åéæ³•å­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼ã€è¿å­—ç¬¦ã€ä¸‹åˆ’çº¿ï¼›è¶…è¿‡ max_length å­—ç¬¦æˆªæ–­ï¼ˆé»˜è®¤ 512ï¼Œå¯åœ¨ config çš„ postprocessing.label_max_length è°ƒæ•´ï¼‰ã€‚"""
    s = re.sub(r'[<>:"/\\|?*\n\r\t]', "_", label)
    s = re.sub(r"\s+", " ", s).strip()
    return s[:max_length] if len(s) > max_length else s


def _sentence_to_keywords(raw: str) -> str:
    """è‹¥æ¨¡å‹è¿”å›å®Œæ•´å¥ï¼ˆå¦‚ The image captures...ï¼‰ï¼Œå°è¯•æå–ä¸ºé€—å·åˆ†éš”çš„å…³é”®è¯ã€‚"""
    s = (raw or "").strip()
    if not s or len(s) > 300:
        return s
    # å·²åƒå…³é”®è¯ï¼šå«é€—å·ä¸”æ— å…¸å‹å¥å­å¼€å¤´
    if "," in s and not re.match(r"^(The|This|It)\s+(image\s+)?(captures|shows|depicts|features)", s, re.I):
        return s
    # å»æ‰å¥é¦– "The image captures/shows ..." ç­‰
    s = re.sub(r"^(The|This)\s+image\s+(captures|shows|depicts|features|presents)\s+", "", s, flags=re.I).strip()
    s = re.sub(r"^(The|This)\s+", "", s, count=1, flags=re.I).strip()
    s = s.rstrip(".")
    # å»æ‰å† è¯ã€ä»‹è¯ç­‰ï¼ŒæŒ‰ç©ºæ ¼/é€—å·æ‹†æˆè¯ï¼Œä¿ç•™æœ‰æ„ä¹‰çš„
    stop = {"a", "an", "the", "in", "on", "at", "is", "are", "of", "to", "and", "or", "for"}
    parts = re.split(r"[\s,]+", s)
    words = [w for w in parts if w and len(w) > 1 and w.lower() not in stop]
    if not words:
        return (raw or "").strip()
    return ", ".join(words[:15])  # æœ€å¤š 15 ä¸ªè¯ä½œä¸ºå…³é”®è¯


def _is_placeholder_caption(text: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸º Step-4 å ä½æè¿°ï¼ˆæ—  VLM æ—¶ç”Ÿæˆï¼‰ï¼Œåº”ä½¿ç”¨ Cluster_N è€ŒéåŸæ–‡ã€‚"""
    s = text.strip()
    if s.startswith("["):
        idx = s.find("]")
        if idx >= 0:
            s = s[idx + 1 :].strip()
    # "Image in cluster 0." / "Image in cluster -1." ç­‰
    return bool(re.match(r"^Image in cluster (-?\d+)\.?$", s, re.I))


def _distill_placeholder(
    cluster_id: int,
    captions: List[str],
    label_length_min: int,
    label_length_max: int,
    label_max_length: int = 512,
) -> str:
    """æ—  LLM æ—¶ï¼šä»é¦–æ¡æè¿°æˆªå–å‰ N è¯ï¼›è‹¥ä¸ºå ä½æè¿°åˆ™ç›´æ¥è¿”å› Cluster_Nã€‚"""
    if not captions:
        return f"Cluster_{cluster_id:02d}"
    first = captions[0].strip()
    if _is_placeholder_caption(first):
        return f"Cluster_{cluster_id:02d}"
    # å»æ‰ [Placeholder] ç­‰å‰ç¼€
    if first.startswith("["):
        idx = first.find("]")
        if idx >= 0:
            first = first[idx + 1 :].strip()
    if _is_placeholder_caption(first):
        return f"Cluster_{cluster_id:02d}"
    words = first.split()
    n = min(label_length_max, max(label_length_min, len(words)))
    label = " ".join(words[:n]) if words else f"Cluster_{cluster_id:02d}"
    return _sanitize_label(label, max_length=label_max_length)


def _merge_keywords(
    keyword_lists: List[str],
    label_max_len: int = 512,
    max_keywords: int = 8,
) -> str:
    """åˆå¹¶å¤šæ¡å…³é”®è¯ä¸²ï¼šæŒ‰é€—å·/ç©ºæ ¼æ‹†è¯ï¼Œå»é‡ï¼ˆä¿åºï¼‰ï¼Œæœ€å¤šä¿ç•™ max_keywords ä¸ªï¼Œå†æ‹¼æˆä¸€æ¡ã€‚"""
    seen: set = set()
    out: List[str] = []
    for s in keyword_lists:
        if not (s or "").strip():
            continue
        s = _sentence_to_keywords(s)
        for part in re.split(r"[\s,]+", s):
            if len(out) >= max_keywords:
                break
            w = part.strip()
            if not w or len(w) < 2:
                continue
            key = w.lower()
            if key in seen:
                continue
            seen.add(key)
            out.append(w)
        if len(out) >= max_keywords:
            break
    return ", ".join(out) if out else ""


def _distill_with_llm(
    cluster_id: int,
    captions: List[str],
    config: dict,
    output_dir: Optional[Path] = None,
) -> str:
    """å…ˆå¯¹æ¯æ¡æè¿°æå–å…³é”®è¯ï¼Œå†åˆå¹¶åŒç°‡å…³é”®è¯ä½œä¸ºç°‡æ ‡ç­¾ï¼›å¤±è´¥åˆ™èµ°å ä½é€»è¾‘ã€‚"""
    post = config.get("postprocessing", {})
    label_min = int(post.get("label_length_min", 5))
    label_max = int(post.get("label_length_max", 10))
    label_max_len = int(post.get("label_max_length", 512))
    label_keyword_max = max(1, int(post.get("label_keyword_max", 8)))
    if not captions:
        return _distill_placeholder(cluster_id, captions, label_min, label_max, label_max_len)
    import sys
    if str(ROOT) not in sys.path:
        sys.path.insert(0, str(ROOT))
    try:
        from models.vlm_models import (
            get_vlm_model_and_processor,
            generate_text,
            is_vlm_available,
        )
    except ImportError as e:
        print(f"[Step-5] å¯¼å…¥ VLM æ¨¡å—å¤±è´¥: {e}")
        return _distill_placeholder(cluster_id, captions, label_min, label_max, label_max_len)
    if not is_vlm_available():
        return _distill_placeholder(cluster_id, captions, label_min, label_max, label_max_len)
    # å•æ¡æè¿°å…³é”®è¯æå– promptï¼Œå ä½ç¬¦ {description}ï¼›æ— åˆ™ç”¨é»˜è®¤
    kw_tpl = (post.get("keyword_extract_prompt") or "").strip()
    if not kw_tpl or "{description}" not in kw_tpl:
        kw_tpl = (
            "Extract 3-8 keywords from the following description. "
            "Reply with words separated by commas only. No sentences.\n\n{description}"
        )
    # è¿‡æ»¤å ä½æè¿°ä¸ç©º
    valid_captions = [
        c.strip() for c in captions
        if (c or "").strip() and not _is_placeholder_caption((c or "").strip())
    ]
    if not valid_captions:
        return _distill_placeholder(cluster_id, captions, label_min, label_max, label_max_len)
    vlm_cfg = config.get("vlm", {})
    try:
        from models.vlm_models import resolve_vlm_model_name
        model_name = resolve_vlm_model_name(config)
    except ImportError:
        model_name = vlm_cfg.get("model_name", "Qwen/Qwen2-VL-2B-Instruct")
    device = (vlm_cfg.get("device") or "cuda").strip().lower() or "cuda"
    try:
        import torch
        if device == "cpu" and torch.cuda.is_available():
            device = "cuda"
    except Exception:
        pass
    quantization = (vlm_cfg.get("quantization") or "none").strip().lower()
    try:
        model, processor = get_vlm_model_and_processor(
            model_name,
            device=device,
            torch_dtype=vlm_cfg.get("torch_dtype", "bfloat16"),
            use_flash_attn=vlm_cfg.get("use_flash_attn", True),
            quantization=quantization if quantization not in ("", "none") else None,
            config=config,
        )
        # é€æ¡æè¿°æå–å…³é”®è¯
        keyword_strs: List[str] = []
        for cap in valid_captions:
            prompt = kw_tpl.replace("{description}", cap)
            raw = generate_text(model, processor, prompt, max_new_tokens=32)
            raw = (raw or "").strip()
            keyword_strs.append(raw or "")
        # æ¯å¥å¾—åˆ°çš„å…³é”®è¯ä¿å­˜åˆ° txtï¼ˆconfig.output.save_keyword_txt ä¸º true æ—¶ï¼‰
        if output_dir and config.get("output", {}).get("save_keyword_txt", True) and keyword_strs:
            kw_dir = output_dir / "step5_keywords"
            kw_dir.mkdir(parents=True, exist_ok=True)
            fname = "noise_keywords.txt" if cluster_id == -1 else f"cluster_{cluster_id:02d}_keywords.txt"
            kw_path = kw_dir / fname
            lines = [f"{kw}\n" for kw in keyword_strs]
            try:
                kw_path.write_text("".join(lines), encoding="utf-8")
            except Exception as e:
                print(f"[Step-5] å†™å…¥ {kw_path} å¤±è´¥: {e}")
        # åˆå¹¶åŒç°‡å…³é”®è¯å¹¶å»é‡ï¼Œæœ€å¤šä¿ç•™ label_keyword_max ä¸ª
        merged = _merge_keywords(keyword_strs, label_max_len, max_keywords=label_keyword_max)
        if merged:
            return _sanitize_label(merged, max_length=label_max_len)
    except Exception as e:
        print(f"[Step-5] ç°‡ {cluster_id} LLM è’¸é¦å¤±è´¥: {e}")
    return _distill_placeholder(cluster_id, captions, label_min, label_max, label_max_len)


def run_step5(
    config: dict,
    captions_path: Path,
    sampled_path: Path,
    output_dir: Path,
    progress_callback=None,
) -> Path:
    """
    è¿è¡Œ Step-5: è¯­ä¹‰è’¸é¦

    Args:
        config: é…ç½®å­—å…¸
        captions_path: S4_captions.json
        sampled_path: S3_sampled_images.json
        output_dir: è¾“å‡ºç›®å½•
        progress_callback: å¯é€‰ï¼Œå›è°ƒ (current, total) ç”¨äº UI è¿›åº¦

    Returns:
        S5_cluster_labels.csv è·¯å¾„
    """
    print("=" * 60)
    print("Step-5: è¯­ä¹‰è’¸é¦")
    print("=" * 60)

    print(f"[Step-5] åŠ è½½ S4 æè¿°ä¸ S3 é‡‡æ ·â€¦")
    with open(captions_path, "r", encoding="utf-8") as f:
        captions_by_image = json.load(f)
    with open(sampled_path, "r", encoding="utf-8") as f:
        sampled = json.load(f)
    post = config.get("postprocessing", {})
    n_clusters = len(sampled)
    label_min = int(post.get("label_length_min", 5))
    label_max = int(post.get("label_length_max", 10))
    label_max_len = int(post.get("label_max_length", 512))
    print(f"[Step-5] å…± {n_clusters} ä¸ªç°‡å¾…è’¸é¦æ ‡ç­¾ï¼ˆlabel_length: {label_min}-{label_max} è¯ï¼Œæœ€å¤§ {label_max_len} å­—ç¬¦ï¼‰")
    rows = []
    for idx, (cid_str, image_ids) in enumerate(sampled.items()):
        cid = int(cid_str)
        captions = [
            captions_by_image.get(iid, "")
            for iid in image_ids
            if captions_by_image.get(iid, "").strip()
        ]
        label = _distill_with_llm(cid, captions, config, output_dir=output_dir)
        if not label:
            label = _distill_placeholder(cid, captions, label_min, label_max, label_max_len)
        rows.append({"cluster_id": cid, "label": label})
        if progress_callback:
            try:
                progress_callback(idx + 1, n_clusters)
            except Exception:
                pass
        if (idx + 1) % max(1, n_clusters // 5) == 0 or idx == n_clusters - 1:
            print(f"[Step-5] å·²è’¸é¦ {idx + 1}/{n_clusters} ç°‡")

    df = pd.DataFrame(rows)
    out_path = output_dir / "S5_cluster_labels.csv"
    df.to_csv(out_path, index=False, encoding="utf-8-sig")
    print(f"[Step-5] Saved {len(rows)} cluster labels -> {out_path.name}")
    print("=" * 60)
    return out_path
