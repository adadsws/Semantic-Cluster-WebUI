"""
VLM/LLM æœ¬åœ°æ¨ç†å°è£…
- Step-4: Qwen2-VL å›¾åƒæè¿°ï¼ˆimage + prompt -> captionï¼‰
- Step-5: åŒä¸€æ¨¡å‹æ–‡æœ¬ç”Ÿæˆï¼ˆcaptions + label_prompt -> cluster labelï¼‰

æ”¯æŒ HuggingFace ä¸ ModelScopeï¼ˆé€šä¹‰åƒé—®2-VLï¼Œè§ https://www.modelscope.cn/models/qwen/Qwen2-VL-2B-Instruct/summaryï¼‰ã€‚
ä½¿ç”¨ transformers AutoProcessor + AutoModelForImageTextToText æœ¬åœ°åŠ è½½ã€‚

ğŸ“… Last Updated: 2026-01-31
"""

import tempfile
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

ROOT = Path(__file__).parent.parent


def _resize_image_for_caption(image_path: Path, max_size: int) -> Tuple[Path, Optional[Path]]:
    """
    æè¿°å‰ç¼©å°å›¾åƒä»¥åŠ é€Ÿï¼šå°†é•¿è¾¹ç¼©è‡³ max_size ä»¥å†…ï¼Œä¿æŒå®½é«˜æ¯”ã€‚
    max_size <= 0 æ—¶ç›´æ¥è¿”å›åŸè·¯å¾„ã€‚

    Returns:
        (path_to_use, temp_path_or_None): è‹¥ç”Ÿæˆäº†ä¸´æ—¶æ–‡ä»¶ï¼Œç¬¬äºŒä¸ªä¸ºä¸´æ—¶è·¯å¾„ï¼ˆè°ƒç”¨æ–¹è´Ÿè´£åˆ é™¤ï¼‰
    """
    if max_size <= 0:
        return image_path, None
    try:
        from PIL import Image
    except ImportError:
        return image_path, None
    try:
        img = Image.open(image_path).convert("RGB")
    except Exception:
        return image_path, None
    w, h = img.size
    if max(w, h) <= max_size:
        return image_path, None
    if w >= h:
        new_w, new_h = max_size, int(h * max_size / w)
    else:
        new_w, new_h = int(w * max_size / h), max_size
    img = img.resize((new_w, new_h), Image.Resampling.LANCZOS)
    fd, temp_path = tempfile.mkstemp(suffix=".jpg", prefix="vlm_resized_")
    import os
    try:
        img.save(temp_path, "JPEG", quality=85)
    finally:
        os.close(fd)
    return Path(temp_path), Path(temp_path)

# å…¨å±€ç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½
_cached_model: Any = None
_cached_processor: Any = None
_cached_model_name: Optional[str] = None

# HuggingFace é»˜è®¤æ¨¡å‹ ID
VLM_MODEL_SMALL = "Qwen/Qwen2-VL-2B-Instruct"
VLM_MODEL_LARGE = "Qwen/Qwen2-VL-7B-Instruct"

# ModelScope é»˜è®¤æ¨¡å‹ IDï¼ˆé€šä¹‰åƒé—®2-VLï¼Œå›½å†…ä¸‹è½½æ›´å¿«ï¼‰
VLM_MODEL_SMALL_MODELSCOPE = "qwen/Qwen2-VL-2B-Instruct"
VLM_MODEL_LARGE_MODELSCOPE = "qwen/Qwen2-VL-7B-Instruct"


def resolve_vlm_model_name(config: dict) -> str:
    """ä» config è§£æå®é™…ä½¿ç”¨çš„ VLM æ¨¡å‹ IDï¼šmodel_scale + model_sourceï¼Œæˆ–æ˜¾å¼ model_nameã€‚"""
    vlm = config.get("vlm", {})
    explicit = (vlm.get("model_name") or "").strip()
    source = (vlm.get("model_source") or "huggingface").strip().lower()
    scale = (vlm.get("model_scale") or "").strip().lower()

    if explicit:
        return explicit
    if source == "modelscope":
        if scale == "small":
            return VLM_MODEL_SMALL_MODELSCOPE
        if scale == "large":
            return VLM_MODEL_LARGE_MODELSCOPE
        return VLM_MODEL_LARGE_MODELSCOPE
    if scale == "small":
        return VLM_MODEL_SMALL
    if scale == "large":
        return VLM_MODEL_LARGE
    return VLM_MODEL_LARGE


def _get_model_load_path(model_id: str, config: dict) -> str:
    """
    æ ¹æ® model_source è¿”å›ç”¨äº from_pretrained çš„è·¯å¾„ï¼š
    - huggingface: ç›´æ¥è¿”å› model_idï¼ˆä» HF ä¸‹è½½ï¼‰
    - modelscope: å…ˆ snapshot_download åˆ°æœ¬åœ°ï¼Œè¿”å›æœ¬åœ°ç›®å½•è·¯å¾„
    """
    source = (config.get("vlm", {}).get("model_source") or "huggingface").strip().lower()
    if source != "modelscope":
        return model_id
    try:
        from modelscope import snapshot_download
    except ImportError:
        print("[VLM] æœªå®‰è£… modelscopeï¼Œä» HuggingFace åŠ è½½ã€‚å¯é€‰: pip install modelscope")
        return model_id
    cache_dir = Path(config.get("system", {}).get("cache_directory", "data/.cache"))
    cache_dir = ROOT / cache_dir if not str(cache_dir).startswith("/") and ":" not in str(cache_dir) else Path(cache_dir)
    cache_dir.mkdir(parents=True, exist_ok=True)
    local_dir = snapshot_download(model_id, cache_dir=str(cache_dir))
    return local_dir


def _get_device_map(device: str) -> str:
    if device == "cuda":
        return "auto"
    if device == "cpu":
        return "cpu"
    return "auto"


def get_vlm_model_and_processor(
    model_name: str,
    device: str = "cuda",
    torch_dtype: Optional[str] = "bfloat16",
    use_flash_attn: bool = True,
    quantization: Optional[str] = None,
    config: Optional[dict] = None,
) -> Tuple[Any, Any]:
    """
    åŠ è½½ VLM æ¨¡å‹ä¸ processorï¼ˆå•ä¾‹ï¼šåŒ model_name+quantization å¤ç”¨ç¼“å­˜ï¼‰ã€‚
    æ”¯æŒ HuggingFaceï¼ˆmodel_idï¼‰ä¸ ModelScopeï¼ˆconfig.vlm.model_source=modelscope æ—¶å…ˆ snapshot_download å†æœ¬åœ°åŠ è½½ï¼‰ã€‚

    Args:
        model_name: HuggingFace æˆ– ModelScope æ¨¡å‹ IDï¼Œå¦‚ Qwen/Qwen2-VL-2B-Instructã€qwen/Qwen2-VL-2B-Instruct
        device: cuda / cpu
        torch_dtype: bfloat16 / float16 / float32ï¼ˆé‡åŒ–æ—¶éƒ¨åˆ†å¿½ç•¥ï¼‰
        use_flash_attn: æ˜¯å¦ä½¿ç”¨ flash_attention_2
        quantization: none / int8 / int4ï¼ˆéœ€å®‰è£… bitsandbytesï¼Œä»… CUDAï¼‰
        config: å¯é€‰ï¼Œå« vlm.model_source æ—¶ä» ModelScope ä¸‹è½½åˆ°æœ¬åœ°å†åŠ è½½

    Returns:
        (model, processor)
    """
    global _cached_model, _cached_processor, _cached_model_name
    q = (quantization or "none").strip().lower()
    cache_key = f"{model_name}|{q}"
    if _cached_model is not None and _cached_model_name == cache_key:
        return _cached_model, _cached_processor

    load_path = _get_model_load_path(model_name, config or {}) if config else model_name
    if load_path != model_name:
        print(f"[VLM] æ­£åœ¨åŠ è½½æ¨¡å‹ï¼ˆModelScope æœ¬åœ°ï¼‰: {model_name} -> {load_path}")
    else:
        print(f"[VLM] æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name} (device={device}, quantization={q})")
    import torch
    from transformers import AutoProcessor
    try:
        from transformers import AutoModelForImageTextToText
        model_cls = AutoModelForImageTextToText
    except ImportError:
        try:
            from transformers import Qwen2VLForConditionalGeneration as model_cls
        except ImportError:
            raise ImportError(
                "éœ€è¦ transformers æ”¯æŒ Qwen2-VLï¼ˆAutoModelForImageTextToText æˆ– Qwen2VLForConditionalGenerationï¼‰"
            )

    device_map = _get_device_map(device)
    kw = {"device_map": device_map, "trust_remote_code": True}

    if q == "int8":
        kw["load_in_8bit"] = True
    elif q == "int4":
        kw["load_in_4bit"] = True
    else:
        dtype_map = {"bfloat16": torch.bfloat16, "float16": torch.float16, "float32": torch.float32}
        dtype = dtype_map.get(torch_dtype, torch.bfloat16)
        # æ–°ç‰ˆæœ¬ transformers æ¨è dtypeï¼Œé¿å… torch_dtype å¼ƒç”¨è­¦å‘Š
        kw["dtype"] = dtype
        if use_flash_attn:
            kw["attn_implementation"] = "flash_attention_2"

    def _load_model():
        return model_cls.from_pretrained(load_path, **kw)

    if q in ("int8", "int4"):
        try:
            model = _load_model()
        except Exception as e:
            raise RuntimeError(
                f"é‡åŒ–åŠ è½½å¤±è´¥ï¼ˆéœ€å®‰è£… bitsandbytes ä¸”ä»… CUDAï¼‰: {e}\n"
                "å¯è®¾ vlm.quantization: none æˆ– pip install bitsandbytes"
            ) from e
    else:
        try:
            model = _load_model()
        except TypeError:
            # æ—§ç‰ˆ transformers åªè®¤ torch_dtype
            kw.pop("dtype", None)
            kw["torch_dtype"] = dtype
            model = _load_model()
        except Exception:
            if use_flash_attn:
                kw.pop("attn_implementation", None)
                print("[VLM] ä½¿ç”¨é»˜è®¤æ³¨æ„åŠ›ï¼ˆå¯é€‰å®‰è£… flash-attn åŠ é€Ÿï¼‰")
                model = _load_model()
            else:
                raise
    print("[VLM] æ­£åœ¨åŠ è½½ processorâ€¦")
    processor = AutoProcessor.from_pretrained(load_path, trust_remote_code=True)

    _cached_model = model
    _cached_processor = processor
    _cached_model_name = cache_key
    print(f"[VLM] æ¨¡å‹ä¸ processor åŠ è½½å®Œæˆï¼ˆå·²ç¼“å­˜ï¼Œåç»­å¤ç”¨ï¼‰")
    return model, processor


def caption_single_image(
    model: Any,
    processor: Any,
    image_path: Path,
    prompt: str,
    max_new_tokens: int = 256,
) -> str:
    """
    å•å¼ å›¾åƒæè¿°ï¼šVLM(image + prompt) -> caption æ–‡æœ¬ã€‚
    ä½¿ç”¨ Qwen2-VL å¯¹è¯æ ¼å¼ï¼šapply_chat_template(conversation) -> generate -> decodeã€‚
    """
    import torch

    path_str = str(image_path.resolve())
    # æœ¬åœ°å›¾ï¼šéƒ¨åˆ†ç‰ˆæœ¬ç”¨ "path"ï¼Œéƒ¨åˆ†ç”¨ "image"ï¼›å…ˆè¯• pathï¼Œå¤±è´¥å†è¯• image
    for img_key in ("path", "image"):
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "image", img_key: path_str},
                    {"type": "text", "text": prompt},
                ],
            }
        ]
        try:
            inputs = processor.apply_chat_template(
                conversation,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt",
            )
            break
        except (TypeError, KeyError, ValueError):
            if img_key == "image":
                raise
            continue
    inputs = {k: v.to(model.device) if hasattr(v, "to") else v for k, v in inputs.items()}
    try:
        from transformers import GenerationConfig
        gen_cfg = GenerationConfig(max_new_tokens=max_new_tokens, do_sample=False)
    except ImportError:
        gen_cfg = None
    with torch.no_grad():
        out = model.generate(**inputs, generation_config=gen_cfg) if gen_cfg else model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
    prompt_len = inputs["input_ids"].shape[1]
    generated = out[:, prompt_len:]
    decoded = processor.batch_decode(generated, skip_special_tokens=True, clean_up_tokenization_spaces=True)
    return (decoded[0] or "").strip()


def _caption_batch_forward(
    model: Any,
    processor: Any,
    paths: List[Path],
    prompt: str,
    max_new_tokens: int,
) -> List[str]:
    """
    å°è¯• processor åŸç”Ÿ batchï¼ˆè‹¥æ”¯æŒå¤š conversation çš„ apply_chat_templateï¼‰ã€‚
    è‹¥ä¸æ”¯æŒæˆ–å¤±è´¥åˆ™è¿”å›ç©ºåˆ—è¡¨ï¼Œç”± caption_batch é€å¼ å›é€€ã€‚
    """
    import torch
    path_strs = [str(p.resolve()) for p in paths]
    conversations = []
    for path_str in path_strs:
        conv = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "path": path_str},
                    {"type": "text", "text": prompt},
                ],
            }
        ]
        conversations.append(conv)
    try:
        inputs = processor.apply_chat_template(
            conversations,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt",
            padding=True,
        )
    except (TypeError, KeyError, ValueError):
        return []
    if not inputs or "input_ids" not in inputs:
        return []
    inputs = {k: v.to(model.device) if hasattr(v, "to") else v for k, v in inputs.items()}
    try:
        from transformers import GenerationConfig
        gen_cfg = GenerationConfig(max_new_tokens=max_new_tokens, do_sample=False)
    except ImportError:
        gen_cfg = None
    with torch.no_grad():
        out = model.generate(**inputs, generation_config=gen_cfg) if gen_cfg else model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
    prompt_len = inputs["input_ids"].shape[1]
    decoded_list = []
    for i in range(out.size(0)):
        generated = out[i : i + 1, prompt_len:]
        dec = processor.batch_decode(generated, skip_special_tokens=True, clean_up_tokenization_spaces=True)
        decoded_list.append((dec[0] or "").strip())
    return decoded_list


def caption_batch(
    model: Any,
    processor: Any,
    items: List[Tuple[str, Path]],
    prompt: str,
    max_new_tokens: int = 256,
    batch_size: int = 4,
    max_image_size: int = 0,
) -> Dict[str, str]:
    """
    æ‰¹é‡å›¾åƒæè¿°ï¼šæŒ‰ batch_size æˆæ‰¹æ¨ç†ï¼Œä¼˜å…ˆå°è¯• batch forwardï¼Œå¤±è´¥åˆ™é€å¼ å›é€€ã€‚
    max_image_size > 0 æ—¶æè¿°å‰å°†å›¾åƒé•¿è¾¹ç¼©è‡³æ­¤åƒç´ ä»¥åŠ é€Ÿï¼ˆä¿æŒå®½é«˜æ¯”ï¼‰ã€‚

    items: [(image_id, path), ...]
    Returns: {image_id: caption, ...}
    """
    import os
    result: Dict[str, str] = {}
    for start in range(0, len(items), batch_size):
        chunk = items[start : start + batch_size]
        iids = [x[0] for x in chunk]
        paths = [x[1] for x in chunk]
        if len(paths) == 0:
            continue
        # å¯é€‰ï¼šç¼©å°åˆ†è¾¨ç‡ä»¥åŠ é€Ÿ
        temp_paths: List[Path] = []
        use_paths: List[Path] = []
        for p in paths:
            use_p, temp_p = _resize_image_for_caption(p, max_image_size)
            use_paths.append(use_p)
            if temp_p is not None:
                temp_paths.append(temp_p)
        try:
            batch_caps = _caption_batch_forward(model, processor, use_paths, prompt, max_new_tokens)
        except Exception:
            batch_caps = []
        if len(batch_caps) == len(iids):
            for iid, cap in zip(iids, batch_caps):
                result[iid] = cap or ""
        else:
            for iid, use_p in zip(iids, use_paths):
                try:
                    cap = caption_single_image(model, processor, use_p, prompt, max_new_tokens=max_new_tokens)
                    result[iid] = cap or ""
                except Exception:
                    result[iid] = ""
        for tp in temp_paths:
            try:
                if tp.exists():
                    os.unlink(tp)
            except Exception:
                pass
    return result


def generate_text(
    model: Any,
    processor: Any,
    prompt: str,
    max_new_tokens: int = 128,
) -> str:
    """
    çº¯æ–‡æœ¬ç”Ÿæˆï¼šLLM(prompt) -> æ–‡æœ¬ï¼ˆç”¨äº Step-5 ç°‡æ ‡ç­¾è’¸é¦ï¼‰ã€‚
    Qwen2-VL çº¯æ–‡æœ¬æ—¶ä»…ä¼  input_ids/attention_maskï¼Œé¿å… apply_chat_template è¿”å›çš„å¤šæ¨¡æ€é”®å¯¼è‡´ string indices é”™è¯¯ã€‚
    """
    import torch

    # Qwen2-VL è¦æ±‚ content ä¸º part åˆ—è¡¨ï¼Œå¦åˆ™ processor éå† content æ—¶ä¼šæŠŠå­—ç¬¦ä¸²å½“è¿­ä»£é¡¹ï¼Œpart["type"] æŠ¥ string indices must be integers
    messages = [{"role": "user", "content": [{"type": "text", "text": prompt}]}]
    raw = processor.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt",
    )
    # åªä¿ç•™å¼ é‡å¹¶ç§»åˆ°è®¾å¤‡ï¼›accept dict æˆ– BatchFeatureï¼ˆtransformers å¯èƒ½è¿”å› BatchFeature è€Œé dictï¼‰
    if not hasattr(raw, "get"):
        raise TypeError(f"apply_chat_template é¢„æœŸè¿”å› dict æˆ– BatchFeatureï¼Œå¾—åˆ° {type(raw)}")
    input_ids = raw.get("input_ids")
    attention_mask = raw.get("attention_mask")
    if input_ids is None:
        raise KeyError("apply_chat_template è¿”å›ä¸­ç¼ºå°‘ input_ids")
    if input_ids.dim() == 1:
        input_ids = input_ids.unsqueeze(0)
    if attention_mask is not None and attention_mask.dim() == 1:
        attention_mask = attention_mask.unsqueeze(0)
    input_ids = input_ids.to(model.device)
    if attention_mask is not None:
        attention_mask = attention_mask.to(model.device)
    kwargs = {"input_ids": input_ids, "max_new_tokens": max_new_tokens, "do_sample": False}
    if attention_mask is not None:
        kwargs["attention_mask"] = attention_mask
    try:
        from transformers import GenerationConfig
        gen_cfg = GenerationConfig(max_new_tokens=max_new_tokens, do_sample=False)
    except ImportError:
        gen_cfg = None
    with torch.no_grad():
        out = model.generate(**kwargs, generation_config=gen_cfg) if gen_cfg else model.generate(**kwargs)
    prompt_len = input_ids.shape[1]
    generated = out[:, prompt_len:]
    decoded = processor.batch_decode(generated, skip_special_tokens=True, clean_up_tokenization_spaces=True)
    return (decoded[0] or "").strip()


def is_vlm_available() -> bool:
    """æ£€æŸ¥å½“å‰ç¯å¢ƒæ˜¯å¦å¯åŠ è½½ VLMï¼ˆtransformers å« Qwen2-VL ç­‰ï¼‰ã€‚"""
    try:
        from transformers import AutoModelForImageTextToText, AutoProcessor
        return True
    except ImportError:
        pass
    try:
        from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
        return True
    except ImportError:
        pass
    try:
        from transformers.models.qwen2_vl import Qwen2VLForConditionalGeneration
        from transformers import AutoProcessor
        return True
    except ImportError:
        pass
    return False


def _get_transformers_version() -> str:
    try:
        import transformers
        return getattr(transformers, "__version__", "æœªçŸ¥")
    except Exception:
        return "æœªå®‰è£…"


def check_vlm_ready(config: dict) -> Tuple[bool, str]:
    """
    å…ˆæ£€æµ‹ VLM æ˜¯å¦å¯ç”¨ä¸”æ¨¡å‹å·²ä¸‹è½½/å¯åŠ è½½ã€‚
    è‹¥å¯ç”¨åˆ™å°è¯•åŠ è½½æ¨¡å‹ï¼ˆåŠ è½½æˆåŠŸä¼šç¼“å­˜ï¼Œåç»­ Step-4 ç›´æ¥å¤ç”¨ï¼‰ã€‚

    Returns:
        (ready, message): ready=True è¡¨ç¤ºå¯ç”¨ä¸”å·²åŠ è½½ï¼›False æ—¶ message è¯´æ˜åŸå› åŠå»ºè®®ã€‚
    """
    if not is_vlm_available():
        ver = _get_transformers_version()
        return (
            False,
            f"VLM ä¸å¯ç”¨ï¼šå½“å‰ transformers ç‰ˆæœ¬ {ver}ï¼Œéœ€ 4.37+ æ‰æ”¯æŒ Qwen2-VLã€‚"
            "è¯·æ‰§è¡Œï¼špip install -U transformers"
        )
    vlm_cfg = config.get("vlm", {})
    model_name = resolve_vlm_model_name(config)
    device = vlm_cfg.get("device", "cuda")
    torch_dtype = vlm_cfg.get("torch_dtype", "bfloat16")
    use_flash_attn = vlm_cfg.get("use_flash_attn", True)
    quantization = (vlm_cfg.get("quantization") or "none").strip().lower()
    try:
        get_vlm_model_and_processor(
            model_name,
            device=device,
            torch_dtype=torch_dtype,
            use_flash_attn=use_flash_attn,
            quantization=quantization if quantization not in ("", "none") else None,
            config=config,
        )
        return (True, f"VLM å·²å°±ç»ªï¼Œæ¨¡å‹å·²åŠ è½½ï¼š{model_name}")
    except Exception as e:
        return (
            False,
            f"æ¨¡å‹æœªä¸‹è½½æˆ–åŠ è½½å¤±è´¥ï¼š{e}\n"
            f"è¯·å…ˆä¸‹è½½æ¨¡å‹ï¼šhuggingface-cli download {model_name}\n"
            "æˆ–æ£€æŸ¥æ˜¾å­˜/ç½‘ç»œåé‡è¯•ã€‚"
        )
